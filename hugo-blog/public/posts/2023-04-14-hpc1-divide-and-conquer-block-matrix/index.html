<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="dark">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>HPC-1-divide-and-conquer-block-matrix-algorithmr | Xi&#39;s Blog</title>
<meta name="keywords" content="matrix-multiplication, cache-optimization, SIMD, intrinsics">
<meta name="description" content="week 2 block matrix algorithm
1. BLIS reference high performance implitation v.s. naive methods:

2. With different block size:
This is the MB NB PB = 40.
But if the block size is too small, the performance is not as good as naive PJI.
The front for loop is JIP is not related to the performance of the algorithm because the computer will focus on each implementation in blocking. That means the register will focus on optimize the final for loop: the Gemm_JPI function, but will not paralize and optimize the for loop for block - matrix- matrix - multiplication.">
<meta name="author" content="Xi Chen">
<link rel="canonical" href="https://xichen1997.github.io/posts/2023-04-14-hpc1-divide-and-conquer-block-matrix/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.56bf4be95d09faf55cd507c845718ccfcabb2e24e37f8fa5a66f9fa098252b06.css" integrity="sha256-Vr9L6V0J&#43;vVc1QfIRXGMz8q7LiTjf4&#43;lpm&#43;foJglKwY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://xichen1997.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://xichen1997.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://xichen1997.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://xichen1997.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://xichen1997.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://xichen1997.github.io/posts/2023-04-14-hpc1-divide-and-conquer-block-matrix/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true
  },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
};

window.addEventListener('load', (event) => {
    document.querySelectorAll("mjx-container").forEach(function(x){
      x.parentElement.classList += 'has-jax'})
  });
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<meta property="og:url" content="https://xichen1997.github.io/posts/2023-04-14-hpc1-divide-and-conquer-block-matrix/">
  <meta property="og:site_name" content="Xi&#39;s Blog">
  <meta property="og:title" content="HPC-1-divide-and-conquer-block-matrix-algorithmr">
  <meta property="og:description" content="week 2 block matrix algorithm 1. BLIS reference high performance implitation v.s. naive methods: 2. With different block size: This is the MB NB PB = 40.
But if the block size is too small, the performance is not as good as naive PJI.
The front for loop is JIP is not related to the performance of the algorithm because the computer will focus on each implementation in blocking. That means the register will focus on optimize the final for loop: the Gemm_JPI function, but will not paralize and optimize the for loop for block - matrix- matrix - multiplication.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-15T00:05:14-04:00">
    <meta property="article:modified_time" content="2024-04-15T00:05:14-04:00">
    <meta property="article:tag" content="Matrix-Multiplication">
    <meta property="article:tag" content="Cache-Optimization">
    <meta property="article:tag" content="SIMD">
    <meta property="article:tag" content="Intrinsics">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="HPC-1-divide-and-conquer-block-matrix-algorithmr">
<meta name="twitter:description" content="week 2 block matrix algorithm
1. BLIS reference high performance implitation v.s. naive methods:

2. With different block size:
This is the MB NB PB = 40.
But if the block size is too small, the performance is not as good as naive PJI.
The front for loop is JIP is not related to the performance of the algorithm because the computer will focus on each implementation in blocking. That means the register will focus on optimize the final for loop: the Gemm_JPI function, but will not paralize and optimize the for loop for block - matrix- matrix - multiplication.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://xichen1997.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "HPC-1-divide-and-conquer-block-matrix-algorithmr",
      "item": "https://xichen1997.github.io/posts/2023-04-14-hpc1-divide-and-conquer-block-matrix/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "HPC-1-divide-and-conquer-block-matrix-algorithmr",
  "name": "HPC-1-divide-and-conquer-block-matrix-algorithmr",
  "description": "week 2 block matrix algorithm 1. BLIS reference high performance implitation v.s. naive methods: 2. With different block size: This is the MB NB PB = 40.\nBut if the block size is too small, the performance is not as good as naive PJI.\nThe front for loop is JIP is not related to the performance of the algorithm because the computer will focus on each implementation in blocking. That means the register will focus on optimize the final for loop: the Gemm_JPI function, but will not paralize and optimize the for loop for block - matrix- matrix - multiplication.\n",
  "keywords": [
    "matrix-multiplication", "cache-optimization", "SIMD", "intrinsics"
  ],
  "articleBody": "week 2 block matrix algorithm 1. BLIS reference high performance implitation v.s. naive methods: 2. With different block size: This is the MB NB PB = 40.\nBut if the block size is too small, the performance is not as good as naive PJI.\nThe front for loop is JIP is not related to the performance of the algorithm because the computer will focus on each implementation in blocking. That means the register will focus on optimize the final for loop: the Gemm_JPI function, but will not paralize and optimize the for loop for block - matrix- matrix - multiplication.\n3. memory and register model. The relation of register and main memory can be shown as above, then we assume that when we computing we can’t transfer data. The cost of transferring data is $R_m$\nThe register can hold 64 doubles(512 bytes)\n4. A example of how register works. 4.1 analysis This is a exmple of using block matrix to update part of the final answer.\nAssuming the CPU needs to spend $\\gamma_R$ time for each floating-point operation and $\\beta_{R \\leftrightarrow M}$ time to transfer data between registers and memory, the total cost for the operation is:\n$$ \\begin{aligned} \u0026 MNK \\left( (m_R n_R + m_R k_R + k_R n_R) \\beta_{R \\leftrightarrow M} + 2 m_R n_R k_R \\gamma_R + m_R n_R \\beta_{R \\leftrightarrow M} \\right) \\\\ \u0026= 2 (M m_R)(N n_R)(K k_R) \\gamma_R + 2 (M m_R)(N n_R) K \\beta_{R \\leftrightarrow M} \\\\ \u0026\\quad + (M m_R) N (K k_R) \\beta_{R \\leftrightarrow M} + M (N n_R)(K k_R) \\beta_{R \\leftrightarrow M} \\\\ \u0026= 2 m n k \\gamma_R + m n k \\left( \\frac{2}{k_R} + \\frac{1}{n_R} + \\frac{1}{m_R} \\right) \\beta_{R \\leftrightarrow M} \\end{aligned} $$\nThe process is:\n$$ \\begin{aligned} \u0026 \\text{for } j = 0, \\ldots, N-1 \\\\ \u0026 \\quad \\text{for } i = 0, \\ldots, M-1 \\\\ \u0026 \\quad \\quad \\text{for } p = 0, \\ldots, K-1 \\\\ \u0026 \\quad \\quad \\quad \\text{Load } C_{i,j}, A_{i,p}, \\text{ and } B_{p,j} \\text{ into registers} \\\\ \u0026 \\quad \\quad \\quad C_{i,j} := A_{i,p} B_{p,j} + C_{i,j} \\\\ \u0026 \\quad \\quad \\quad \\text{Store } C_{i,j} \\text{ to memory} \\end{aligned} $$\nBut we could do better by saving $C_{ij}$ before the P loop:\n$$ \\begin{aligned} \u0026 \\text{for } j = 0, \\ldots, N-1 \\\\ \u0026 \\quad \\text{for } i = 0, \\ldots, M-1 \\\\ \u0026 \\quad \\quad \\text{Load } C_{i,j} \\\\ \u0026 \\quad \\quad \\text{for } p = 0, \\ldots, K-1 \\\\ \u0026 \\quad \\quad \\quad \\text{Load } A_{i,p} \\text{ and } B_{p,j} \\text{ into registers} \\\\ \u0026 \\quad \\quad \\quad C_{i,j} := A_{i,p} B_{p,j} + C_{i,j} \\\\ \u0026 \\quad \\quad \\text{Store } C_{i,j} \\end{aligned} $$\nThen the final computational time is:\n$$ \\begin{aligned} \u0026 MNK(2m_Rn_Rk_R)\\gamma_R + [MN(2m_Rn_R) + MNK(m_Rk_R + k_Rn_R)]\\beta_{R \\leftrightarrow M} \\\\ \u0026= 2mnk\\gamma_R + \\left[2mn + mnk\\left(\\frac{1}{n_R} + \\frac{1}{m_R}\\right)\\right]\\beta_{R \\leftrightarrow M} \\end{aligned} $$ The capital K is Larger, the saved time is more.\n4.2 Streaming operation:JIP_P_Ger The picture show how we do the streaming, the nature of this operation is to use rank-1 operation to update the block matrix, the rank-1 operation will continue update matrix C without changing data with main memory.\nEvery time we do streaming operation:\nIn order to make good use of all the register memory, we can assume that $m_R$, $n_R$, and $k_R$ are the same, and assume they are 4. The overall memory in register is $$ m_R \\times n_R + m_R + n_R = 24 $$\nBut the performance (use block size of 4) is not soo good.\tthe picture will show the block size is 400. which is much better.\n4.3 Micro Kernel :JIP_P_Ger In fact in the operation of streaming operation, we don’t need to split the p into pk, and we can save the time for for loop:\n5. Optimize the micro kernel 5.1 vector register FMA: fused multiple add operation\nSIMD: simple instruction multiple data\n$$ \\begin{pmatrix} \\gamma_{0,0} \u0026 \\gamma_{0,1} \u0026 \\gamma_{0,2} \u0026 \\gamma_{0,3} \\\\ \\gamma_{1,0} \u0026 \\gamma_{1,1} \u0026 \\gamma_{1,2} \u0026 \\gamma_{1,3} \\\\ \\gamma_{2,0} \u0026 \\gamma_{2,1} \u0026 \\gamma_{2,2} \u0026 \\gamma_{2,3} \\\\ \\gamma_{3,0} \u0026 \\gamma_{3,1} \u0026 \\gamma_{3,2} \u0026 \\gamma_{3,3} \\end{pmatrix} +:= \\begin{pmatrix} \\alpha_{0,p} \\\\ \\alpha_{1,p} \\\\ \\alpha_{2,p} \\\\ \\alpha_{3,p} \\end{pmatrix} \\begin{pmatrix} \\beta_{p,0} \u0026 \\beta_{p,1} \u0026 \\beta_{p,2} \u0026 \\beta_{p,3} \\end{pmatrix} $$\n$$ \\beta_{p,0} \\begin{pmatrix} \\alpha_{0,p} \\\\ \\alpha_{1,p} \\\\ \\alpha_{2,p} \\\\ \\alpha_{3,p} \\end{pmatrix} + \\beta_{p,1} \\begin{pmatrix} \\alpha_{0,p} \\\\ \\alpha_{1,p} \\\\ \\alpha_{2,p} \\\\ \\alpha_{3,p} \\end{pmatrix} + \\beta_{p,2} \\begin{pmatrix} \\alpha_{0,p} \\\\ \\alpha_{1,p} \\\\ \\alpha_{2,p} \\\\ \\alpha_{3,p} \\end{pmatrix} + \\beta_{p,3} \\begin{pmatrix} \\alpha_{0,p} \\\\ \\alpha_{1,p} \\\\ \\alpha_{2,p} \\\\ \\alpha_{3,p} \\end{pmatrix} $$\nIf we use SIMD:\n$$ \\begin{array}{|c|} \\hline \\gamma_{0,0} \\\\ \\hline \\gamma_{1,0} \\\\ \\hline \\gamma_{2,0} \\\\ \\hline \\gamma_{3,0} \\\\ \\hline \\end{array} \\quad \\begin{array}{c} +:= \\\\ +:= \\\\ +:= \\\\ +:= \\end{array} \\quad \\begin{array}{|c|} \\hline \\alpha_{0,p} \\\\ \\hline \\alpha_{1,p} \\\\ \\hline \\alpha_{2,p} \\\\ \\hline \\alpha_{3,p} \\\\ \\hline \\end{array} \\quad \\begin{array}{c} \\times \\\\ \\times \\\\ \\times \\\\ \\times \\end{array} \\quad \\begin{array}{|c|} \\hline \\beta_{p,0} \\\\ \\hline \\beta_{p,0} \\\\ \\hline \\beta_{p,0} \\\\ \\hline \\beta_{p,0} \\\\ \\hline \\end{array} $$\nThen implement it use the code: using AX2 intrinsic.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 #define alpha( i,j ) A[ (j)*ldA + (i) ] // map alpha( i,j ) to array A #define beta( i,j ) B[ (j)*ldB + (i) ] // map beta( i,j ) to array B #define gamma( i,j ) C[ (j)*ldC + (i) ] // map gamma( i,j ) to array C #include void Gemm_MRxNRKernel( int k, double *A, int ldA, double *B, int ldB, double *C, int ldC ) { /* Declare vector registers to hold 4x4 C and load them */ __m256d gamma_0123_0 = _mm256_loadu_pd( \u0026gamma( 0,0 ) ); __m256d gamma_0123_1 = _mm256_loadu_pd( \u0026gamma( 0,1 ) ); __m256d gamma_0123_2 = _mm256_loadu_pd( \u0026gamma( 0,2 ) ); __m256d gamma_0123_3 = _mm256_loadu_pd( \u0026gamma( 0,3 ) ); for ( int p=0; p\u003ck; p++ ){ /* Declare vector register for load/broadcasting beta( p,j ) */ __m256d beta_p_j; /* Declare a vector register to hold the current column of A and load it with the four elements of that column. */ __m256d alpha_0123_p = _mm256_loadu_pd( \u0026alpha( 0,p ) ); /* Load/broadcast beta( p,0 ). */ beta_p_j = _mm256_broadcast_sd( \u0026beta( p, 0) ); /* update the first column of C with the current column of A times beta ( p,0 ) */ gamma_0123_0 = _mm256_fmadd_pd( alpha_0123_p, beta_p_j, gamma_0123_0 ); /* REPEAT for second, third, and fourth columns of C. Notice that the current column of A needs not be reloaded. */ beta_p_j = _mm256_broadcast_sd( \u0026beta( p, 1) ); gamma_0123_1 = _mm256_fmadd_pd( alpha_0123_p, beta_p_j, gamma_0123_1 ); beta_p_j = _mm256_broadcast_sd( \u0026beta( p, 2) ); gamma_0123_2 = _mm256_fmadd_pd( alpha_0123_p, beta_p_j, gamma_0123_2 ); beta_p_j = _mm256_broadcast_sd( \u0026beta( p, 3) ); gamma_0123_3 = _mm256_fmadd_pd( alpha_0123_p, beta_p_j, gamma_0123_3 ); } /* Store the updated results */ _mm256_storeu_pd( \u0026gamma(0,0), gamma_0123_0 ); _mm256_storeu_pd( \u0026gamma(0,1), gamma_0123_1 ); _mm256_storeu_pd( \u0026gamma(0,2), gamma_0123_2 ); _mm256_storeu_pd( \u0026gamma(0,3), gamma_0123_3 ); } 1 2 3 4 5 6 7 8 __m256d gamma_0123_0 = _mm256_loadu_pd \\\\\\\\ This load the vector which contains 4 doubles (note the intrinsic can only hold 4 doubles) beta_p_j = _mm256_broadcast_sd( \u0026beta( p, 0) );\\\\\\\\ This use one double to do broadcast. gamma_0123_0 = _mm256_fmadd_pd( alpha_0123_p, beta_p_j, gamma_0123_0 );\\\\\\\\ This will use FMA to compute from two vectors and accumulate the result _mm256_storeu_pd( \u0026gamma(0,0), gamma_0123_0 );\\\\\\\\ This will return the memory from register to main-memory. 5.2 Best performance analysis of the data transferring cost when the register memory is limited. S is for the C matrix, M is for the matrix of B and A. Because in the end we will send all C matrix parts into register memory, so the total number of this is |C| we don’t need to analysis. As for the B and A, because of the streaming data, we should analyze them carefully.\nWe can combine the 3 for loop and split them into different phases. at each phase, we send S+M data into the register memory. S is storage of fast memory, and M is the data need to be replaced next phase.\n$$ \\begin{aligned} \u0026 \\text{for } r = 0, \\ldots, mnk-1 \\\\ \u0026 \\quad \\gamma_{i_r,j_r} := \\alpha_{i_r,p_r} \\beta_{p_r,j_r} + \\gamma_{i_r,j_r} \\\\ \u0026 \\text{end} \\end{aligned} $$\nThe overall FMAs is mnk in this equation, we assume we can have Fmax FMAs by sending the memory into register once, then we need to transfer data:\n$$ \\begin{equation*} \\left(\\frac{m n k}{F_{\\rm max}} -1 \\right) M \\end{equation*} $$\nSo if we choose S and M properly, we can maximize F and get the good solution of all the equation. In order to maximize F, we could use a model to abstract this procedure:\nFor a each phase, we assume a space D combined with (ir,jr,pr) tuple, In the subspace of AD, we have (ir,pr), for the subspace of BD, we have (jr, pr), in the subspace of CD, we have (ir,jr). ( r is the index of one phase.)\nThe number of the space D can be limited by this relation(3d geometry):\n$$ \\vert \\mathbf{D} \\vert \\leq \\sqrt{\\vert \\mathbf{C_D} \\vert \\vert \\mathbf{A_D} \\vert \\vert \\mathbf{B_D} \\vert }\\text{.} $$\nSo we could limit D by these inequal:\n$$ \\begin{equation*} {\\rm maximize~} F_{\\rm max} {\\rm suchthat~} \\left{ \\begin{array}{l} F_{\\rm max} \\leq \\sqrt{\\vert \\mathbf{C_D} \\vert \\vert \\mathbf{A_D} \\vert \\vert \\mathbf{B_D} \\vert } \\\\ \\vert \\mathbf{C_D} \\vert \\gt 0, \\vert \\mathbf{A_D} \\vert \\gt 0, \\vert \\mathbf{B_D} \\vert \\gt 0 \\\\ \\vert \\mathbf{C_D} \\vert + \\vert \\mathbf{A_D} \\vert + \\vert \\mathbf{B_D} \\vert = S + M. \\end{array} \\right. \\end{equation*} $$\nAccording to mathematical calculation, the:\n$$ \\begin{aligned} |\\mathbf{C_D}| = |\\mathbf{A_D}| = |\\mathbf{B_D}| \u0026= \\frac{S+M}{3} \\\\ F_{\\max} \u0026= \\frac{(S + M)\\sqrt{S+M}}{3\\sqrt{3}} \\end{aligned} $$\nAnd:\n$$ \\begin{equation*} \\left(\\frac{m n k}{F_{\\rm max}} -1 \\right) M = \\left(3 \\sqrt{3} \\frac{m n k}{( S + M )\\sqrt{S+M}} -1 \\right) M. \\end{equation*} $$\nTake the derivatives of the right hand side, we assume S is a const and M is a variable, so in the end we get:\n$$ M=2S $$\n$$ \\begin{equation*} \\left(3 \\sqrt{3} \\frac{m n k}{( 3 S )\\sqrt{3 S}} -1 \\right) (2S) = 2 \\frac{m n k}{\\sqrt{S}} - 2S. \\end{equation*} $$\nSatisfy the equation, and S+M \u003c= register memory, the S is bigger, the performance is better. and M = 2S. If we take an example, the register can hold 32 doubles, so S+M = 32, and with M = 2S, we get S = 32/3 ≈ 10.67 doubles for storage.\nAll the information are from the paper: https://arxiv.org/pdf/1702.02017.pdf\nAppendix I 1 __m256d _mm256_loadu_pd (double const * mem_addr) Description\nLoad 256-bits (composed of 4 packed double-precision (64-bit) floating-point elements) from memory into dst (output). mem_addr does not need to be aligned on any particular boundary.\n1 __m256d _mm256_broadcast_sd (double const * mem_addr) Description\nBroadcast a double-precision (64-bit) floating-point element from memory to all elements of dst (output).\n1 __m256d _mm256_fmadd_pd (__m256d a, __m256d b, __m256d c) Description\nMultiply packed double-precision (64-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst (output).\nAppendix II Intel intrinsics reference:\nhttps://software.intel.com/sites/landingpage/IntrinsicsGuide/\n",
  "wordCount" : "1852",
  "inLanguage": "en",
  "datePublished": "2024-04-15T00:05:14-04:00",
  "dateModified": "2024-04-15T00:05:14-04:00",
  "author":{
    "@type": "Person",
    "name": "Xi Chen"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://xichen1997.github.io/posts/2023-04-14-hpc1-divide-and-conquer-block-matrix/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Xi's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://xichen1997.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://xichen1997.github.io/" accesskey="h" title="Xi&#39;s Blog (Alt + H)">Xi&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://xichen1997.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://xichen1997.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://xichen1997.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://xichen1997.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://xichen1997.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      HPC-1-divide-and-conquer-block-matrix-algorithmr
    </h1>
    <div class="post-meta"><span title='2024-04-15 00:05:14 -0400 EDT'>April 15, 2024</span>&nbsp;·&nbsp;<span>9 min</span>&nbsp;·&nbsp;<span>1852 words</span>&nbsp;·&nbsp;<span>Xi Chen</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#41-analysis">4.1 analysis</a></li>
    <li><a href="#42--streaming-operationjip_p_ger">4.2  Streaming operation:JIP_P_Ger</a></li>
    <li><a href="#43-micro-kernel-jip_p_ger">4.3 Micro Kernel :JIP_P_Ger</a></li>
  </ul>

  <ul>
    <li><a href="#51-vector-register">5.1 vector register</a></li>
    <li><a href="#52-best-performance-analysis-of-the-data-transferring-cost-when-the-register-memory-is-limited">5.2 Best performance analysis of the data transferring cost when the register memory is limited.</a></li>
    <li><a href="#appendix-i">Appendix I</a></li>
    <li><a href="#appendix-ii">Appendix II</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="week-2-block-matrix-algorithm">week 2 block matrix algorithm<a hidden class="anchor" aria-hidden="true" href="#week-2-block-matrix-algorithm">#</a></h1>
<h1 id="1-blis-reference-high-performance-implitation-vs-naive-methods">1. BLIS reference high performance implitation v.s. naive methods:<a hidden class="anchor" aria-hidden="true" href="#1-blis-reference-high-performance-implitation-vs-naive-methods">#</a></h1>
<p><img alt="img" loading="lazy" src="https://raw.githubusercontent.com/xichen1997/picture_for_blog/master/Plot_All_Orderings.png"></p>
<h1 id="2-with-different-block-size">2. With different block size:<a hidden class="anchor" aria-hidden="true" href="#2-with-different-block-size">#</a></h1>
<p>This is the MB NB PB = 40.</p>
<p>But if the block size is too small, the performance is not as good as naive PJI.</p>
<p>The front for loop is JIP is not related to the performance of the algorithm because the computer will focus on each implementation in blocking. That means the register will focus on optimize the final for loop: the Gemm_JPI function, but will not paralize and optimize the for loop for block - matrix- matrix - multiplication.</p>
<p><img alt="img" loading="lazy" src="https://raw.githubusercontent.com/xichen1997/picture_for_blog/master/Plot_Blocked_MMM.png"></p>
<h1 id="3-memory-and-register-model">3. memory and register model.<a hidden class="anchor" aria-hidden="true" href="#3-memory-and-register-model">#</a></h1>
<p><img alt="img" loading="lazy" src="https://raw.githubusercontent.com/xichen1997/picture_for_blog/master/20241025123736.png"></p>
<p>The relation of register and main memory can be shown as above, then we assume that when we computing we can&rsquo;t transfer data. The cost of transferring data is $R_m$</p>
<p>The register can hold 64 doubles(512 bytes)</p>
<h1 id="4-a-example-of-how-register-works">4. A example of how register works.<a hidden class="anchor" aria-hidden="true" href="#4-a-example-of-how-register-works">#</a></h1>
<h2 id="41-analysis">4.1 analysis<a hidden class="anchor" aria-hidden="true" href="#41-analysis">#</a></h2>
<p><img alt="img" loading="lazy" src="http://www.cs.utexas.edu/users/flame/laff/pfhp/images/Week2/GemmIJPIJP.png"></p>
<p>This is a exmple of using block matrix to update part of the final answer.</p>
<p>Assuming the CPU needs to spend $\gamma_R$ time for each floating-point operation and $\beta_{R \leftrightarrow M}$ time to transfer data between registers and memory, the total cost for the operation is:</p>
<p>$$
\begin{aligned}
&amp; MNK \left( (m_R n_R + m_R k_R + k_R n_R) \beta_{R \leftrightarrow M} + 2 m_R n_R k_R \gamma_R + m_R n_R \beta_{R \leftrightarrow M} \right) \\
&amp;= 2 (M m_R)(N n_R)(K k_R) \gamma_R + 2 (M m_R)(N n_R) K \beta_{R \leftrightarrow M} \\
&amp;\quad + (M m_R) N (K k_R) \beta_{R \leftrightarrow M} + M (N n_R)(K k_R) \beta_{R \leftrightarrow M} \\
&amp;= 2 m n k \gamma_R + m n k \left( \frac{2}{k_R} + \frac{1}{n_R} + \frac{1}{m_R} \right) \beta_{R \leftrightarrow M}
\end{aligned}
$$</p>
<p>The process is:</p>
<p>$$
\begin{aligned}
&amp; \text{for } j = 0, \ldots, N-1 \\
&amp; \quad \text{for } i = 0, \ldots, M-1 \\
&amp; \quad \quad \text{for } p = 0, \ldots, K-1 \\
&amp; \quad \quad \quad \text{Load } C_{i,j}, A_{i,p}, \text{ and } B_{p,j} \text{ into registers} \\
&amp; \quad \quad \quad C_{i,j} := A_{i,p} B_{p,j} + C_{i,j} \\
&amp; \quad \quad \quad \text{Store } C_{i,j} \text{ to memory}
\end{aligned}
$$</p>
<p>But we could do better by saving $C_{ij}$ before the P loop:</p>
<p>$$
\begin{aligned}
&amp; \text{for } j = 0, \ldots, N-1 \\
&amp; \quad \text{for } i = 0, \ldots, M-1 \\
&amp; \quad \quad \text{Load } C_{i,j} \\
&amp; \quad \quad \text{for } p = 0, \ldots, K-1 \\
&amp; \quad \quad \quad \text{Load } A_{i,p} \text{ and } B_{p,j} \text{ into registers} \\
&amp; \quad \quad \quad C_{i,j} := A_{i,p} B_{p,j} + C_{i,j} \\
&amp; \quad \quad \text{Store } C_{i,j}
\end{aligned}
$$</p>
<p>Then the final computational time is:</p>
<p>$$
\begin{aligned}
&amp; MNK(2m_Rn_Rk_R)\gamma_R + [MN(2m_Rn_R) + MNK(m_Rk_R + k_Rn_R)]\beta_{R \leftrightarrow M} \\
&amp;= 2mnk\gamma_R + \left[2mn + mnk\left(\frac{1}{n_R} + \frac{1}{m_R}\right)\right]\beta_{R \leftrightarrow M}
\end{aligned}
$$
The capital K is Larger, the saved time is more.</p>
<h2 id="42--streaming-operationjip_p_ger">4.2  Streaming operation:JIP_P_Ger<a hidden class="anchor" aria-hidden="true" href="#42--streaming-operationjip_p_ger">#</a></h2>
<p>The picture show how we do the streaming, the nature of this operation is to use rank-1 operation to update the block matrix, the rank-1 operation will continue update matrix C without changing data with main memory.</p>
<p><img alt="img" loading="lazy" src="http://www.cs.utexas.edu/users/flame/laff/pfhp/images/Week2/GemmIJPPrank1.png"></p>
<p>Every time we do streaming operation:</p>
<p><img alt="img" loading="lazy" src="http://www.cs.utexas.edu/users/flame/laff/pfhp/images/Week2/GemmIJPPrank1One.png"></p>
<p>In order to make good use of all the register memory, we can assume that $m_R$, $n_R$, and $k_R$ are the same, and assume they are 4. The overall memory in register is
$$
m_R \times n_R + m_R + n_R  = 24
$$</p>
<p><img alt="plot_register_block_4" loading="lazy" src="http://xcwp.azurewebsites.net/wp-content/uploads/2020/05/plot_register_block_4.png"></p>
<p>But the performance (use block size of 4) is not soo good.	the picture will show the block size is 400. which is much better.</p>
<p><img alt="Plot_register_blocking_400" loading="lazy" src="http://xcwp.azurewebsites.net/wp-content/uploads/2020/05/Plot_register_blocking_400.png"></p>
<h2 id="43-micro-kernel-jip_p_ger">4.3 Micro Kernel :JIP_P_Ger<a hidden class="anchor" aria-hidden="true" href="#43-micro-kernel-jip_p_ger">#</a></h2>
<p>In fact in the operation of streaming operation, we don&rsquo;t need to split the p into pk, and we can save the time for for loop:</p>
<p><img alt="img" loading="lazy" src="http://www.cs.utexas.edu/users/flame/laff/pfhp/images/Week2/GemmIJPPrank1Two.png"></p>
<h1 id="5-optimize-the-micro-kernel">5. Optimize the micro kernel<a hidden class="anchor" aria-hidden="true" href="#5-optimize-the-micro-kernel">#</a></h1>
<h2 id="51-vector-register">5.1 vector register<a hidden class="anchor" aria-hidden="true" href="#51-vector-register">#</a></h2>
<p>FMA: fused multiple add operation</p>
<p>SIMD: simple instruction multiple data</p>
<p>$$
\begin{pmatrix}
\gamma_{0,0} &amp; \gamma_{0,1} &amp; \gamma_{0,2} &amp; \gamma_{0,3} \\
\gamma_{1,0} &amp; \gamma_{1,1} &amp; \gamma_{1,2} &amp; \gamma_{1,3} \\
\gamma_{2,0} &amp; \gamma_{2,1} &amp; \gamma_{2,2} &amp; \gamma_{2,3} \\
\gamma_{3,0} &amp; \gamma_{3,1} &amp; \gamma_{3,2} &amp; \gamma_{3,3}
\end{pmatrix}
+:=
\begin{pmatrix}
\alpha_{0,p} \\
\alpha_{1,p} \\
\alpha_{2,p} \\
\alpha_{3,p}
\end{pmatrix}
\begin{pmatrix}
\beta_{p,0} &amp; \beta_{p,1} &amp; \beta_{p,2} &amp; \beta_{p,3}
\end{pmatrix}
$$</p>
<p>$$
\beta_{p,0}
\begin{pmatrix}
\alpha_{0,p} \\
\alpha_{1,p} \\
\alpha_{2,p} \\
\alpha_{3,p}
\end{pmatrix}
+
\beta_{p,1}
\begin{pmatrix}
\alpha_{0,p} \\
\alpha_{1,p} \\
\alpha_{2,p} \\
\alpha_{3,p}
\end{pmatrix}
+
\beta_{p,2}
\begin{pmatrix}
\alpha_{0,p} \\
\alpha_{1,p} \\
\alpha_{2,p} \\
\alpha_{3,p}
\end{pmatrix}
+
\beta_{p,3}
\begin{pmatrix}
\alpha_{0,p} \\
\alpha_{1,p} \\
\alpha_{2,p} \\
\alpha_{3,p}
\end{pmatrix}
$$</p>
<p>If we use SIMD:</p>
<p>$$
\begin{array}{|c|}
\hline
\gamma_{0,0} \\ \hline
\gamma_{1,0} \\ \hline
\gamma_{2,0} \\ \hline
\gamma_{3,0} \\ \hline
\end{array}
\quad
\begin{array}{c}
+:= \\
+:= \\
+:= \\
+:=
\end{array}
\quad
\begin{array}{|c|}
\hline
\alpha_{0,p} \\ \hline
\alpha_{1,p} \\ \hline
\alpha_{2,p} \\ \hline
\alpha_{3,p} \\ \hline
\end{array}
\quad
\begin{array}{c}
\times \\
\times \\
\times \\
\times
\end{array}
\quad
\begin{array}{|c|}
\hline
\beta_{p,0} \\ \hline
\beta_{p,0} \\ \hline
\beta_{p,0} \\ \hline
\beta_{p,0} \\ \hline
\end{array}
$$</p>
<p>Then implement it use the code: using AX2 intrinsic.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C++" data-lang="C++"><span class="line"><span class="cl"><span class="cp">#define alpha( i,j ) A[ (j)*ldA + (i) ]   </span><span class="c1">// map alpha( i,j ) to array A
</span></span></span><span class="line"><span class="cl"><span class="cp">#define beta( i,j )  B[ (j)*ldB + (i) ]   </span><span class="c1">// map beta( i,j ) to array B
</span></span></span><span class="line"><span class="cl"><span class="cp">#define gamma( i,j ) C[ (j)*ldC + (i) ]   </span><span class="c1">// map gamma( i,j ) to array C
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#include&lt;immintrin.h&gt;
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">Gemm_MRxNRKernel</span><span class="p">(</span> <span class="kt">int</span> <span class="n">k</span><span class="p">,</span> <span class="kt">double</span> <span class="o">*</span><span class="n">A</span><span class="p">,</span> <span class="kt">int</span> <span class="n">ldA</span><span class="p">,</span> <span class="kt">double</span> <span class="o">*</span><span class="n">B</span><span class="p">,</span> <span class="kt">int</span> <span class="n">ldB</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="kt">double</span> <span class="o">*</span><span class="n">C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">ldC</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="cm">/* Declare vector registers to hold 4x4 C and load them */</span>
</span></span><span class="line"><span class="cl">  <span class="n">__m256d</span> <span class="n">gamma_0123_0</span> <span class="o">=</span> <span class="n">_mm256_loadu_pd</span><span class="p">(</span> <span class="o">&amp;</span><span class="n">gamma</span><span class="p">(</span> <span class="mi">0</span><span class="p">,</span><span class="mi">0</span> <span class="p">)</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">__m256d</span> <span class="n">gamma_0123_1</span> <span class="o">=</span> <span class="n">_mm256_loadu_pd</span><span class="p">(</span> <span class="o">&amp;</span><span class="n">gamma</span><span class="p">(</span> <span class="mi">0</span><span class="p">,</span><span class="mi">1</span> <span class="p">)</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">__m256d</span> <span class="n">gamma_0123_2</span> <span class="o">=</span> <span class="n">_mm256_loadu_pd</span><span class="p">(</span> <span class="o">&amp;</span><span class="n">gamma</span><span class="p">(</span> <span class="mi">0</span><span class="p">,</span><span class="mi">2</span> <span class="p">)</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">__m256d</span> <span class="n">gamma_0123_3</span> <span class="o">=</span> <span class="n">_mm256_loadu_pd</span><span class="p">(</span> <span class="o">&amp;</span><span class="n">gamma</span><span class="p">(</span> <span class="mi">0</span><span class="p">,</span><span class="mi">3</span> <span class="p">)</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">   	
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span> <span class="kt">int</span> <span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">p</span><span class="o">&lt;</span><span class="n">k</span><span class="p">;</span> <span class="n">p</span><span class="o">++</span> <span class="p">){</span>
</span></span><span class="line"><span class="cl">    <span class="cm">/* Declare vector register for load/broadcasting beta( p,j ) */</span>
</span></span><span class="line"><span class="cl">    <span class="n">__m256d</span> <span class="n">beta_p_j</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="cm">/* Declare a vector register to hold the current column of A and load
</span></span></span><span class="line"><span class="cl"><span class="cm">       it with the four elements of that column. */</span>
</span></span><span class="line"><span class="cl">    <span class="n">__m256d</span> <span class="n">alpha_0123_p</span> <span class="o">=</span> <span class="n">_mm256_loadu_pd</span><span class="p">(</span> <span class="o">&amp;</span><span class="n">alpha</span><span class="p">(</span> <span class="mi">0</span><span class="p">,</span><span class="n">p</span> <span class="p">)</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="cm">/* Load/broadcast beta( p,0 ). */</span>
</span></span><span class="line"><span class="cl">    <span class="n">beta_p_j</span> <span class="o">=</span> <span class="n">_mm256_broadcast_sd</span><span class="p">(</span> <span class="o">&amp;</span><span class="n">beta</span><span class="p">(</span> <span class="n">p</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="cm">/* update the first column of C with the current column of A times
</span></span></span><span class="line"><span class="cl"><span class="cm">       beta ( p,0 ) */</span>
</span></span><span class="line"><span class="cl">    <span class="n">gamma_0123_0</span> <span class="o">=</span> <span class="n">_mm256_fmadd_pd</span><span class="p">(</span> <span class="n">alpha_0123_p</span><span class="p">,</span> <span class="n">beta_p_j</span><span class="p">,</span> <span class="n">gamma_0123_0</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="cm">/* REPEAT for second, third, and fourth columns of C.  Notice that the 
</span></span></span><span class="line"><span class="cl"><span class="cm">       current column of A needs not be reloaded. */</span>
</span></span><span class="line"><span class="cl">    <span class="n">beta_p_j</span> <span class="o">=</span> <span class="n">_mm256_broadcast_sd</span><span class="p">(</span> <span class="o">&amp;</span><span class="n">beta</span><span class="p">(</span> <span class="n">p</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">gamma_0123_1</span> <span class="o">=</span> <span class="n">_mm256_fmadd_pd</span><span class="p">(</span> <span class="n">alpha_0123_p</span><span class="p">,</span> <span class="n">beta_p_j</span><span class="p">,</span> <span class="n">gamma_0123_1</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">beta_p_j</span> <span class="o">=</span> <span class="n">_mm256_broadcast_sd</span><span class="p">(</span> <span class="o">&amp;</span><span class="n">beta</span><span class="p">(</span> <span class="n">p</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">gamma_0123_2</span> <span class="o">=</span> <span class="n">_mm256_fmadd_pd</span><span class="p">(</span> <span class="n">alpha_0123_p</span><span class="p">,</span> <span class="n">beta_p_j</span><span class="p">,</span> <span class="n">gamma_0123_2</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">beta_p_j</span> <span class="o">=</span> <span class="n">_mm256_broadcast_sd</span><span class="p">(</span> <span class="o">&amp;</span><span class="n">beta</span><span class="p">(</span> <span class="n">p</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">gamma_0123_3</span> <span class="o">=</span> <span class="n">_mm256_fmadd_pd</span><span class="p">(</span> <span class="n">alpha_0123_p</span><span class="p">,</span> <span class="n">beta_p_j</span><span class="p">,</span> <span class="n">gamma_0123_3</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="cm">/* Store the updated results */</span>
</span></span><span class="line"><span class="cl">  <span class="n">_mm256_storeu_pd</span><span class="p">(</span> <span class="o">&amp;</span><span class="n">gamma</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">gamma_0123_0</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">_mm256_storeu_pd</span><span class="p">(</span> <span class="o">&amp;</span><span class="n">gamma</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">gamma_0123_1</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">_mm256_storeu_pd</span><span class="p">(</span> <span class="o">&amp;</span><span class="n">gamma</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">gamma_0123_2</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">_mm256_storeu_pd</span><span class="p">(</span> <span class="o">&amp;</span><span class="n">gamma</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">gamma_0123_3</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C++" data-lang="C++"><span class="line"><span class="cl"><span class="n">__m256d</span> <span class="n">gamma_0123_0</span> <span class="o">=</span> <span class="n">_mm256_loadu_pd</span> <span class="err">\\\\</span> <span class="n">This</span> <span class="n">load</span> <span class="n">the</span> <span class="n">vector</span> <span class="n">which</span> <span class="n">contains</span> <span class="mi">4</span> <span class="n">doubles</span> <span class="p">(</span><span class="n">note</span> <span class="n">the</span> <span class="n">intrinsic</span> <span class="n">can</span> <span class="n">only</span> <span class="n">hold</span> <span class="mi">4</span> <span class="n">doubles</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="n">beta_p_j</span> <span class="o">=</span> <span class="n">_mm256_broadcast_sd</span><span class="p">(</span> <span class="o">&amp;</span><span class="n">beta</span><span class="p">(</span> <span class="n">p</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="p">);</span><span class="err">\\\\</span> <span class="n">This</span> <span class="n">use</span> <span class="n">one</span> <span class="kt">double</span> <span class="n">to</span> <span class="k">do</span> <span class="n">broadcast</span><span class="p">.</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">   <span class="n">gamma_0123_0</span> <span class="o">=</span> <span class="n">_mm256_fmadd_pd</span><span class="p">(</span> <span class="n">alpha_0123_p</span><span class="p">,</span> <span class="n">beta_p_j</span><span class="p">,</span> <span class="n">gamma_0123_0</span> <span class="p">);</span><span class="err">\\\\</span> <span class="n">This</span> <span class="n">will</span> <span class="n">use</span> <span class="n">FMA</span> <span class="n">to</span> <span class="n">compute</span> <span class="n">from</span> <span class="n">two</span> <span class="n">vectors</span> <span class="n">and</span> <span class="n">accumulate</span> <span class="n">the</span> <span class="n">result</span> 
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="n">_mm256_storeu_pd</span><span class="p">(</span> <span class="o">&amp;</span><span class="n">gamma</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">gamma_0123_0</span> <span class="p">);</span><span class="err">\\\\</span> <span class="n">This</span> <span class="n">will</span> <span class="k">return</span> <span class="n">the</span> <span class="n">memory</span> <span class="n">from</span> <span class="k">register</span> <span class="n">to</span> <span class="n">main</span><span class="o">-</span><span class="n">memory</span><span class="p">.</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="52-best-performance-analysis-of-the-data-transferring-cost-when-the-register-memory-is-limited">5.2 Best performance analysis of the data transferring cost when the register memory is limited.<a hidden class="anchor" aria-hidden="true" href="#52-best-performance-analysis-of-the-data-transferring-cost-when-the-register-memory-is-limited">#</a></h2>
<p>S is for the C matrix, M is for the matrix of B and A. Because in the end we will send all C matrix parts into register memory, so the total number of this is |C| we don&rsquo;t need to analysis. As for the B and A, because of the streaming data, we should analyze them carefully.</p>
<p>We can combine the 3 for loop and split them into different phases. at each phase, we send S+M data into the register memory. S is storage of fast memory, and M is the data need to be replaced next phase.</p>
<p>$$
\begin{aligned}
&amp; \text{for } r = 0, \ldots, mnk-1 \\
&amp; \quad \gamma_{i_r,j_r} := \alpha_{i_r,p_r} \beta_{p_r,j_r} + \gamma_{i_r,j_r} \\
&amp; \text{end}
\end{aligned}
$$</p>
<p>The overall FMAs is mnk in this equation, we assume we can have Fmax FMAs by sending the memory into register once, then we need to transfer data:</p>
<p>$$
\begin{equation*}
\left(\frac{m n k}{F_{\rm max}} -1 \right) M
\end{equation*}
$$</p>
<p>So if we choose S and M properly, we can maximize F and get the good solution of all the equation. In order to maximize F, we could use a model to abstract this procedure:</p>
<p>For a each phase, we assume a space D combined with (ir,jr,pr) tuple, In the subspace of AD, we have (ir,pr), for the subspace of BD, we have (jr, pr), in the subspace of CD, we have (ir,jr). ( r is the index of one phase.)</p>
<p>The number of the space D can be limited by this relation(3d geometry):</p>
<p>$$
\vert \mathbf{D} \vert \leq \sqrt{\vert
\mathbf{C_D} \vert \vert \mathbf{A_D} \vert \vert \mathbf{B_D}
\vert }\text{.}
$$</p>
<p>So we could limit D by these inequal:</p>
<p>$$
\begin{equation*}
{\rm maximize~} F_{\rm max} {\rm <del>such</del>that~} \left{
\begin{array}{l} F_{\rm max} \leq \sqrt{\vert \mathbf{C_D}
\vert \vert \mathbf{A_D} \vert \vert \mathbf{B_D} \vert } \\
\vert \mathbf{C_D} \vert \gt 0, \vert \mathbf{A_D} \vert \gt
0, \vert \mathbf{B_D} \vert \gt 0 \\ \vert \mathbf{C_D}
\vert + \vert \mathbf{A_D} \vert + \vert \mathbf{B_D} \vert
= S + M.  \end{array} \right.
\end{equation*}
$$</p>
<p>According to mathematical calculation, the:</p>
<p>$$
\begin{aligned}
|\mathbf{C_D}| = |\mathbf{A_D}| = |\mathbf{B_D}| &amp;= \frac{S+M}{3} \\
F_{\max} &amp;= \frac{(S + M)\sqrt{S+M}}{3\sqrt{3}}
\end{aligned}
$$</p>
<p>And:</p>
<p>$$
\begin{equation*}
\left(\frac{m n k}{F_{\rm max}} -1 \right) M =
\left(3 \sqrt{3} \frac{m n k}{( S + M )\sqrt{S+M}} -1 \right) M.
\end{equation*}
$$</p>
<p>Take the derivatives of the right hand side, we assume S is a const and M is a variable, so in the end we get:</p>
<p>$$
M=2S
$$</p>
<p>$$
\begin{equation*}
\left(3 \sqrt{3} \frac{m n k}{( 3 S  )\sqrt{3 S}} -1 \right) (2S)
= 2 \frac{m n k}{\sqrt{S}} - 2S.
\end{equation*}
$$</p>
<p>Satisfy the equation, and S+M &lt;= register memory, the S is bigger, the performance is better. and M = 2S. If we take an example, the register can hold 32 doubles, so S+M = 32, and with M = 2S, we get S = 32/3 ≈ 10.67 doubles for storage.</p>
<p>All the information are from the paper:
<a href="https://arxiv.org/pdf/1702.02017.pdf">https://arxiv.org/pdf/1702.02017.pdf</a></p>
<h2 id="appendix-i">Appendix I<a hidden class="anchor" aria-hidden="true" href="#appendix-i">#</a></h2>
<ul>
<li>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C++" data-lang="C++"><span class="line"><span class="cl"><span class="n">__m256d</span> <span class="n">_mm256_loadu_pd</span> <span class="p">(</span><span class="kt">double</span> <span class="k">const</span> <span class="o">*</span> <span class="n">mem_addr</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Description</p>
<p>Load 256-bits (composed of 4 packed double-precision (64-bit) floating-point elements) from memory into dst (output). mem_addr does not need to be aligned on any particular boundary.</p>
</li>
<li></li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C++" data-lang="C++"><span class="line"><span class="cl"><span class="n">__m256d</span> <span class="n">_mm256_broadcast_sd</span> <span class="p">(</span><span class="kt">double</span> <span class="k">const</span> <span class="o">*</span> <span class="n">mem_addr</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Description</p>
<p>Broadcast a double-precision (64-bit) floating-point element from memory to all elements of dst (output).</p>
<ul>
<li></li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C++" data-lang="C++"><span class="line"><span class="cl"><span class="n">__m256d</span> <span class="n">_mm256_fmadd_pd</span> <span class="p">(</span><span class="n">__m256d</span> <span class="n">a</span><span class="p">,</span> <span class="n">__m256d</span> <span class="n">b</span><span class="p">,</span> <span class="n">__m256d</span> <span class="n">c</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Description</p>
<p>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst (output).</p>
<h2 id="appendix-ii">Appendix II<a hidden class="anchor" aria-hidden="true" href="#appendix-ii">#</a></h2>
<p>Intel intrinsics reference:</p>
<p><a href="https://software.intel.com/sites/landingpage/IntrinsicsGuide/">https://software.intel.com/sites/landingpage/IntrinsicsGuide/</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://xichen1997.github.io/tags/matrix-multiplication/">Matrix-Multiplication</a></li>
      <li><a href="https://xichen1997.github.io/tags/cache-optimization/">Cache-Optimization</a></li>
      <li><a href="https://xichen1997.github.io/tags/simd/">SIMD</a></li>
      <li><a href="https://xichen1997.github.io/tags/intrinsics/">Intrinsics</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://xichen1997.github.io/posts/2024-04-16-hpc3-openmp-shared-memory-method/">
    <span class="title">« Prev</span>
    <br>
    <span>HPC-3-use-openmp(shared-memory-method)</span>
  </a>
  <a class="next" href="https://xichen1997.github.io/posts/2023-04-15-hpc2-memory-hierarchy/">
    <span class="title">Next »</span>
    <br>
    <span>HPC-2-Memory-hierarchy-in-computer</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://xichen1997.github.io/">Xi&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
