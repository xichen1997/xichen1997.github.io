<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="dark">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Kernel comparison with a MMA in CUDA and near-SOTA/cuBLAS performance kernel | Xi&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="Kernel comparison with a MMA in CUDA and near-SOTA/cuBLAS performance kernel
The project is hosted in the repository:
CUDA-refresh
Introduction
The kernel is the &ldquo;kernel&rdquo; in the concept of CUDA, it directly influence the compute efficiency and it&rsquo;s the key to take advanage of GPU&rsquo;s huge amount of computation resource and bandwidth.
Here is a simple refresh of the CUDA calculation and memory hierachy and their infleunce to the computation efficiency.">
<meta name="author" content="Xi Chen">
<link rel="canonical" href="https://xichen1997.github.io/posts/2025-11-17-introduction-to-cuda-and-high-performance-cuda-kernels/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.56bf4be95d09faf55cd507c845718ccfcabb2e24e37f8fa5a66f9fa098252b06.css" integrity="sha256-Vr9L6V0J&#43;vVc1QfIRXGMz8q7LiTjf4&#43;lpm&#43;foJglKwY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://xichen1997.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://xichen1997.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://xichen1997.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://xichen1997.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://xichen1997.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://xichen1997.github.io/posts/2025-11-17-introduction-to-cuda-and-high-performance-cuda-kernels/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true
  },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
};

window.addEventListener('load', (event) => {
    document.querySelectorAll("mjx-container").forEach(function(x){
      x.parentElement.classList += 'has-jax'})
  });
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<meta property="og:url" content="https://xichen1997.github.io/posts/2025-11-17-introduction-to-cuda-and-high-performance-cuda-kernels/">
  <meta property="og:site_name" content="Xi&#39;s Blog">
  <meta property="og:title" content="Kernel comparison with a MMA in CUDA and near-SOTA/cuBLAS performance kernel">
  <meta property="og:description" content="Kernel comparison with a MMA in CUDA and near-SOTA/cuBLAS performance kernel The project is hosted in the repository: CUDA-refresh
Introduction The kernel is the “kernel” in the concept of CUDA, it directly influence the compute efficiency and it’s the key to take advanage of GPU’s huge amount of computation resource and bandwidth.
Here is a simple refresh of the CUDA calculation and memory hierachy and their infleunce to the computation efficiency.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-11-16T00:06:00+00:00">
    <meta property="article:modified_time" content="2025-11-16T00:06:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kernel comparison with a MMA in CUDA and near-SOTA/cuBLAS performance kernel">
<meta name="twitter:description" content="Kernel comparison with a MMA in CUDA and near-SOTA/cuBLAS performance kernel
The project is hosted in the repository:
CUDA-refresh
Introduction
The kernel is the &ldquo;kernel&rdquo; in the concept of CUDA, it directly influence the compute efficiency and it&rsquo;s the key to take advanage of GPU&rsquo;s huge amount of computation resource and bandwidth.
Here is a simple refresh of the CUDA calculation and memory hierachy and their infleunce to the computation efficiency.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://xichen1997.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Kernel comparison with a MMA in CUDA and near-SOTA/cuBLAS performance kernel",
      "item": "https://xichen1997.github.io/posts/2025-11-17-introduction-to-cuda-and-high-performance-cuda-kernels/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Kernel comparison with a MMA in CUDA and near-SOTA/cuBLAS performance kernel",
  "name": "Kernel comparison with a MMA in CUDA and near-SOTA\/cuBLAS performance kernel",
  "description": "Kernel comparison with a MMA in CUDA and near-SOTA/cuBLAS performance kernel The project is hosted in the repository: CUDA-refresh\nIntroduction The kernel is the \u0026ldquo;kernel\u0026rdquo; in the concept of CUDA, it directly influence the compute efficiency and it\u0026rsquo;s the key to take advanage of GPU\u0026rsquo;s huge amount of computation resource and bandwidth.\nHere is a simple refresh of the CUDA calculation and memory hierachy and their infleunce to the computation efficiency.\n",
  "keywords": [
    
  ],
  "articleBody": "Kernel comparison with a MMA in CUDA and near-SOTA/cuBLAS performance kernel The project is hosted in the repository: CUDA-refresh\nIntroduction The kernel is the “kernel” in the concept of CUDA, it directly influence the compute efficiency and it’s the key to take advanage of GPU’s huge amount of computation resource and bandwidth.\nHere is a simple refresh of the CUDA calculation and memory hierachy and their infleunce to the computation efficiency.\nIn the tutorial, several concepts will be compare like $threads | blocks$ numbers, warp, shared memory, and double buffering(not finished yet).\nSetup before doing the matrix computation. In this section, multiple benchmark will be applied on different kernels like the performance, including kernel Size, matrix size and the way how we use memory(shared memory or L2 cache, etc)\nWe only focus on the kernel of MMA so the main functions are almost the same in each kernels’ benchmark.\nYou can check the main.cu in each steps folder to check how the benchmark is run. They should be pretty straight forward. And if you want to test, make sure you have an NVIDIA GPU(The current compilation only support RTX 30 series and 40 series, if you want backward compatibility please change the compliation switch -arch=sm89 to previous version.)\nbenchmark.sh will be used to measure the time spent on creating matrix and cuda computation seperately by timing. You can run make benchmark to get the logs/ folder and check its running result with different size of matrix.\nIn the heavy profiling stage we will use ncu to do the benchmark, it can show us how much % we use for the GPU’s SM units, please refer to the documentation at Nvidia. Call it with make benchmark-heavy\nNaive kernel - write first kernel function in CUDA In this section, the MMA(matrix multiplication) is implemented in a very simple way. Compared to write in CPU, in GPU, we parallelize the first two loops and each thread calculate the result for a single element in C. $$C[i][j] = \\sum_k A[i][k] * B[k][j]$$\nIt’s easy to write the kernel: for each thread, we calculate the row and col accoding to the threadIdx.x and threadIdx.y.\nNote: The size of the blockDim.x * blockDim.y * blockDim.z \u003c= 2048(phsical threads limitation). But the Grid size can be set to a large number like 64k(It’s just a logic dimension to distribute different tasks to different threads).\nEach threads or warp(a group of threads which will be covered later) will gover a certain range of the problems which can be parallel, like in the matrix multiplication, naive kernel each thread will be responsible for calculate one element in C use A[i][:] * B[:][j].\nIn the main.cu file in step-1, we can use the matrix dimension to decide the grid size:\n1 2 3 4 // make sure one block is multiple of 32 for better performance, 32 threads is in a warp dim3 blockSize(32, 32); dim3 gridSize((N + blockSize.x - 1) / blockSize.x, (M + blockSize.y - 1) / blockSize.y); // always generate more than enough to prevent the task scale is not enough to cover the C matrix scale. In this place I use 32x32 = 1024 threads in each block.\nTips: the CUDA hardware and memory hierachy, use RTX 4090(ada arch) as an example GPU has more parallel computing capacity compared to CPU. In CUDA, the calculation style is SIMT(single instructions multiple threads), compared to SIMD(single instruction multiple data), each threads has their own registers.\nAnd 32 threads are grouped together to execute the same instruction together, so the minimal schedule unit is warp not thread in the GPU.\nThe warps/threads can be logically grouped into a block, the threads number must be a multiple of 32. And SM(Streaming Multiprocessor) can execute the block, one SM has certain numbers of hardware like shared memory and registers, which will decide how many blocks can run on a SM. (note 1 block can only be a resident in 1 SM)\nFor example, Ada Lovelace GPU(RTX 4090) has 128 SMs. Each SM has 64k registers and 100KB shared memory(shared with L1 cache), 2048 threads. That’s the phsical limitation. Note, L2 cache is shared by all SMs, not exclusive to one SM.\nAnd each SM has 4 warp schedulers, which means it can issue 4 instruction to warps at the same cycle. Thus, block at least should have 4 warps to maxmize the performance of each SM.\nThe GPU is connected to HBM(24GB for RTX 4090) which bandwidth reaches 1008GB/s. That’s the phsical limitation of the data flow and decide how many calculation we can do.\nThen we should take a look into how the block size and grid size to match the matrix size: In CUDA, for the row we calculate use row = threadIdx.y + blockIdx.y * blockDim.y and col = threadIdx.x + blockIdx.x * blockDim.x if we use 2D representation because that’s the rules in CUDA. That’s might be unintuitive at the beginning but you can think in this way: the thread should be continous along x then y, so in the matrix, the matrix is row-major, so the changing of x reflect the change of col. They are trying to improve cache locality.\nBack to the naive kernel After introduce the memory layout and rules it should be easy to write the kernel:\n1 2 3 4 5 6 7 8 9 10 11 12 __global__ void kernel1(const float* A, const float* B, float* C, const int M, const int N, const int K){ int row = blockIdx.y * blockDim.y + threadIdx.y; int col = blockIdx.x * blockDim.x + threadIdx.x; float sum = 0.0f; if(row \u003c M \u0026\u0026 col \u003c K){ // prevent over the boundary for(int i = 0; i \u003c N; ++i){ sum += A[row * N + i] * B[i * K + col]; } } C[row * N + col] = sum; } After running the make benchmark we can check the result in the log, for the 8192 * 8192 matrix size, the computational timing and allocation timings are:\n1 2 3 4 5 6 7 Matrix size: 8192x8192 * 8192x8192 ------------------time used ---------------------- Initialization time: 0.457656 seconds Computation time: 0.256279 seconds -------------------------------------------------- Can we make it better? The answer is yes, recall how the array is stored in C++. It’s row major, which means the memory address is continuous for each row.\n1 2 3 int arr[2][4] = { {1,2,3,4} ,{ 5,6,7,8} }; // a[0][0] a[0][1] a[0][2] a[0][3] they are continuous. // \u0026a[1][0] - \u0026a[0][0] = 4(stride) So why don’t we store B in col-major? It means we still take the logic B[row][col] but they are not align with their phsical location. Assuming the B is KxN matrix, then the B[row][col] = B'[col * K + row] in 1D.\nThe experienment can be done quickly in step-2, we change how we allocate memory to the matrix B(transpose it before do cuda memory copy):\n1 2 3 4 5 for(int i = 0; i \u003c K; ++i){ for(int j = 0; j \u003c N; ++j){ h_B[j * K + i] = static_cast\u003cfloat\u003e(i * K + j); } } remember we transpose B already so $B[j][i] = B^T[i][j] = B’[i*K+j]$ $$ C_{ij} = \\sum_k A_{ik} * B_{kj} = \\sum_k A[row * K + i] * B[col * K + j] $$\nThe kernel is:\n1 2 3 4 5 6 7 8 9 10 11 12 __global__ void kernel2(const float* A, const float* B, float* C, const int M, const int N, const int K){ int row = blockIdx.y * blockDim.y + threadIdx.y; int col = blockIdx.x * blockDim.x + threadIdx.x; float sum = 0.0f; if(row \u003c M \u0026\u0026 col \u003c K){ for(int i = 0; i \u003c N; ++i){ sum += A[row * K + i] * B[col * K + i]; } } C[row * K + col] = sum; } That’s should work, let’s see the result for the 8192 dimension compare to naive kernel:\n1 2 3 4 5 6 7 Matrix size: 8192x8192 * 8192x8192 ------------------time used ---------------------- Initialization time: 0.554599 seconds Computation time: 1.65866 seconds -------------------------------------------------- Wait?! Why it needs more time than naive kernel???\nanalysis of memory access pattern We know the cache locality will define the calculation, why the pattern fail in this case?\nThe answer is we are using a lot of different threads to calculate it, each threads represents a different row, col. Inside a warp, all the threads ids are continuous, they are accessing the same row and different cols.\nAnd inside a kernel, at the same time they are accessing the data: sum += A[row * K + i] * B[col * K + i];, different threads will use different col, and their stride is K which is 8192 and this cause all the threads could reuse data another threads access.\nWarp has a feature called data coelesing, which means if threads inside a warp are accessing the continuous memory, they can combine the requests into a single one.\nThat explains why the performance dropped a lot, it improves the single threads efficiency but require too much bandwidth.\nTry 1D access pattern Open step-4 to do same experiment with step-1 but represent the matrix in 1D, you will find the performance improve a little bit, but not too much\nTiled-matrix: Improve the data transfer efficiency to boost computation. After we know the properties of warp and block(all the threads in a block can use the same shared_memory), we can conside data reuse.\nRecall what we have done in the naive kernel: $$C[i][j] = \\sum_k A[i][k] * B[k][j]$$\neach thread use their own memory, which requires a row of A and a col of B, which is a huge waste. What can we do better? Tiled memory calculate, assuming we are taking a band of 16 x K of A and K x 16 of B to calculate the 16x16 of C_blocks matrix.\nOf course we are using block matrix multiplication, we can’t store the two bands of matrix together but when we iterate over K dimension, assuming the stride is 16, then we need a loop for K / 16 times, and each time we put the 16x16 A and 16x16 B tiles into shared memory to do mma add to C block(16x16).\nHow many times we can reuse it? 16 * 16 * 16 (calculation FLOPS) / 2 * 16 * 16 (the A tile and B tile) = 8 times, that’s a huge boost, we can extend the number to 32, 64, etc, with larger tile. However, the tile can not expand infinitely because of the limitation of 100KB limitation(SRAM is very expensive!).\nThe best size in RTX 4090 is about 128x128 or 256 * 64, you can run and compare the step-6 and step-5 for comparison, of course in 5 the block size is 16 x 16 and it cause extra overhead to move the data into the shared memory and that offset the benefit of data reuse.\nHere is the kernel for 32 x 32 C block size, you can try different block size and benchmark thems.\nNotice the loading process is complete by all the threads in the blocks. You can use the size of A or B blocks / threads number to decide which thread load a certain or several elements of a matrix.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 #include #include // Kernel function to do naive matrix multiplication // A[i][j] = i * N + j; // B[i][j] = i * K + j; # define TILE 32 __global__ void kernel_tile(const float* A, const float* B, float* C, const int M, const int N, const int K){ // inside of a tile, local index int tx = threadIdx.x; int ty = threadIdx.y; // block index int bx = blockIdx.x; int by = blockIdx.y; // global row and col index int row = by * TILE + ty; int col = bx * TILE + tx; float sum = 0.0f; __shared__ float As[TILE][TILE]; __shared__ float Bs[TILE][TILE]; // loop over all tiles for(int t = 0; t \u003c (K + TILE - 1) / TILE; ++t){ // load A and B tiles into shared memory // each thread loads one element of each tile if(row \u003c M \u0026\u0026 t * TILE + tx \u003c K){ As[ty][tx] = A[row * K + t * TILE + tx]; }else{ As[ty][tx] = 0.0f; } if(col \u003c N \u0026\u0026 t * TILE + ty \u003c K){ Bs[ty][tx] = B[(t * TILE + ty) * K + col]; }else{ Bs[ty][tx] = 0.0f; } __syncthreads(); // compute partial sum for this tile for(int i = 0; i \u003c TILE; ++i){ sum += As[ty][i] * Bs[i][tx]; } __syncthreads(); } // write back the result if(row \u003c M \u0026\u0026 col \u003c N){ C[row * N + col] = sum; } } Warp level matrix multiplication - the secrets of new GPU architecture Currently the CUDA cores (which are the normal ALU elements) in the SM can do a lot of operations, but 90% of calculation in deep learning are matrix multiplication. Thus, a new warp-level of hardware is introduced, tensor core, we need use warp to schedule it.\nWithin a cycle, normal warp can do 64 FLOPS, but tensor core can do 512 FLOPS for a 16 x 16 matrix. (which is 8x boost.)\nTo use the kernel, we need to define the fragments(16x16) of A, B and C:\n1 2 3 4 // define tensor core fragment wmma::fragment\u003cwmma::matrix_a, TILE, TILE, TILE, half, wmma::row_major\u003e a_frag; wmma::fragment\u003cwmma::matrix_b, TILE, TILE, TILE, half, wmma::row_major\u003e b_frag; wmma::fragment\u003cwmma::accumulator, TILE, TILE, TILE, float\u003e c_frag; F At this stage, we should divide a sub sub matrix in C for the warp to control, for simplicity, we assume the block size is 16 x 16 and the warp control all the block, then we can have the kernel to boost the 16 x 16 tiled matrix multiplication by:\n$$C_sub = A[16][:] @ B[:][16] = A[16][0..15] @ [0..15][16] + A[16][16..31] @ [16..31][16] + …$$\nHere is the kernel:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 #include #include #include // for wmma #include // for half using namespace nvcuda; // Kernel function to do naive matrix multiplication // A[i][j] = i * N + j; // B[i][j] = i * K + j; # define TILE 16 __global__ void kernel_tile(const half * A, const half * B, float* C, const int M, const int N, const int K){ // define tensor core fragment wmma::fragment\u003cwmma::matrix_a, TILE, TILE, TILE, half, wmma::row_major\u003e a_frag; wmma::fragment\u003cwmma::matrix_b, TILE, TILE, TILE, half, wmma::row_major\u003e b_frag; wmma::fragment\u003cwmma::accumulator, TILE, TILE, TILE, float\u003e c_frag; // initialization of C fragment wmma::fill_fragment(c_frag, 0.0f); int block_row = blockIdx.y; int block_col = blockIdx.x; const half *A_ptr = A + block_row * TILE * K; const half *B_ptr = B + block_col * TILE; for(int i = 0; i \u003c K; i += TILE){ // load the intput matrices A and B to fragments wmma::load_matrix_sync(a_frag, A_ptr, K); wmma::load_matrix_sync(b_frag, B_ptr, N); // perform the matrix multiplication wmma::mma_sync(c_frag, a_frag, b_frag, c_frag); } // calculate the starting row and column index of C matrix float * c_ptr = C + block_row * TILE * N + block_col * TILE; // write memory back to C // fix the error: no matching function for call to 'store_matrix_sync' wmma::store_matrix_sync(c_ptr, c_frag, N, wmma::mem_row_major); } Final steps - combine warp level matrix multiplication and shared memory In this way, we know how to use data better to boost the bandwidth. Then the wmma can be used with shared memory, in this case we use 128 x 128 x 32 kernel (A tile is 128 x 32, B tile is 32 x 128 and C tile is 128 x 128 to save shared memory). The shared memory usage is 128 * 32 * 2 = 8192 elements and 16KB usage(for half data type is 2 Bytes) which is pretty good, it will not pressure L1 cache a lot and can hold 2-4 blocks in a SM.\nHere is the kernel, notice each block control 128 x 128 matrix, and 16 warps will consumes this. Each warp get 32 x 32 warp level tile matrix, it should be divided into 4 fragments matrix and do the calculation. Here is the kernel, you can also try it in step-12.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 #include #include #include // for wmma #include // for half using namespace nvcuda; // warp size wichi is constant 16 x 16 x 16 #define WARP_M 16 #define WARP_N 16 #define WARP_K 16 // block tile size #define BLOCK_M 128 // 8 * WARP_M #define BLOCK_N 128 // 8 * WARP_N #define BLOCK_K 32 // 8 * WARP_K // #define WARPS_PER_BLOCK_M 4 // block number of warps in M dimension #define WARPS_PER_BLOCK_N 4 // block number of warps in N dimension #define BLOCK_WARPS (WARPS_PER_BLOCK_M * WARPS_PER_BLOCK_N) // 4 * 4 = 16 warps per block #define BLOCK_THREADS (32 * BLOCK_WARPS) // 32 * 16 = 512 threads per block // the govern region of C_blocks for each warp #define WARP_C_TILE_M (BLOCK_M / WARPS_PER_BLOCK_M) // 128 / 4 = 32 #define WARP_C_TILE_N (BLOCK_N / WARPS_PER_BLOCK_N) // 128 / 4 = 32 // number of fragments of C for each warp #define WARP_C_FRAGS_M (WARP_C_TILE_M / WARP_M) // 32 / 16 = 2 #define WARP_C_FRAGS_N (WARP_C_TILE_N / WARP_N) // 32 / 16 = 2 __global__ void kernel_smem_tile_128x128(const half * A, const half * B, float* C, const int M, const int N, const int K) { // A: (M, K) row-major // B: (K, N) col-major // C: (M, N) row-major // shared memory size for A and B tiles 4096 + 4,096 = 8,192 elements // 16KB in total. // A tile: (BLOCK_M, BLOCK_K) -\u003e (128, 32) = 4096 __shared__ half smem_a[BLOCK_M * BLOCK_K]; // B tile: (BLOCK_K, BLOCK_N) -\u003e (32, 128) = 4096 __shared__ half smem_b[BLOCK_K * BLOCK_N]; // fragments of A and B wmma::fragment\u003cwmma::matrix_a, WARP_M, WARP_N, WARP_K, half, wmma::row_major\u003e a_frag; wmma::fragment\u003cwmma::matrix_b, WARP_M, WARP_N, WARP_K, half, wmma::col_major\u003e b_frag; // fragmesnts of C (in total 8 fragments per warp) wmma::fragment\u003cwmma::accumulator, WARP_M, WARP_N, WARP_K, float\u003e c_frag[WARP_C_FRAGS_M][WARP_C_FRAGS_N]; // initialization of C fragments in each warp for (int m = 0; m \u003c WARP_C_FRAGS_M; ++m) { for (int n = 0; n \u003c WARP_C_FRAGS_N; ++n) { wmma::fill_fragment(c_frag[m][n], 0.0f); } } // 4 x 4 warps per block int warpId = threadIdx.y; // 0..15 int laneId = threadIdx.x; // 0..31 int block_row_base = blockIdx.y * BLOCK_M; int block_col_base = blockIdx.x * BLOCK_N; // warpId 0, 32, 64 int warp_row_offset = (warpId / WARPS_PER_BLOCK_N) * WARP_C_TILE_M; // 0, 32, 64 int warp_col_offset = (warpId % WARPS_PER_BLOCK_N) * WARP_C_TILE_N; // thread ID within the block for loading shared memory int tid = threadIdx.x + threadIdx.y * blockDim.x; // 0..511 for (int k_tile_idx = 0; k_tile_idx \u003c K; k_tile_idx += BLOCK_K) { const half *A_gmem_ptr = A + block_row_base * K + k_tile_idx; const half *B_gmem_ptr = B + block_col_base * K + k_tile_idx; // A (row-major) load for (int i = 0; i \u003c (BLOCK_M * BLOCK_K / BLOCK_THREADS); ++i) { // 4096 / 512 = 8 int idx = tid + i * BLOCK_THREADS; int row = idx / BLOCK_K; int col = idx % BLOCK_K; smem_a[idx] = A_gmem_ptr[row * K + col]; } // B (col-major) load for (int i = 0; i \u003c (BLOCK_K * BLOCK_N / BLOCK_THREADS); ++i) { // 4096 / 512 = 8 int idx = tid + i * BLOCK_THREADS; int row = idx / BLOCK_N; // (k-dim) int col = idx % BLOCK_N; // (n-dim) smem_b[col * BLOCK_K + row] = B_gmem_ptr[col * K + row]; } __syncthreads(); // K-loop (inner): BLOCK_K (32) / WARP_K (16) = 2 for(int inner_k = 0; inner_k \u003c BLOCK_K; inner_k += WARP_K){ // calculate all fragments of A(16 x 32) and B(32 x 16) for this warp to compute a C fragment(16x16) // so there are total 2 times of wmma iuoperations per for (int m = 0; m \u003c WARP_C_FRAGS_M; ++m) { // 0.. for (int n = 0; n \u003c WARP_C_FRAGS_N; ++n) { // 0..1 // A tile for this warp starts at smem_a[warp_row_offset * BLOCK_K] // M-dim offset: m * WARP_M const half * A_smem_ptr = \u0026smem_a[ (warp_row_offset + m * WARP_M) * BLOCK_K + inner_k ]; // B tile for this warp starts at smem_b[warp_col_offset * BLOCK_K] // N-dim offset: n * WARP_N const half * B_smem_ptr = \u0026smem_b[ (warp_col_offset + n * WARP_N) * BLOCK_K + inner_k ]; wmma::load_matrix_sync(a_frag, A_smem_ptr, BLOCK_K); // LDM=BLOCK_K (32) wmma::load_matrix_sync(b_frag, B_smem_ptr, BLOCK_K); // LDM=BLOCK_K (32) // accumulate wmma::mma_sync(c_frag[m][n], a_frag, b_frag, c_frag[m][n]); } } } __syncthreads(); // synchronize before loading the next tile } // write back C fragments to global memory float * c_ptr_base = C + (block_row_base + warp_row_offset) * N + block_col_base + warp_col_offset; // write back all C fragments of this warp for (int m = 0; m \u003c WARP_C_FRAGS_M; ++m) { for (int n = 0; n \u003c WARP_C_FRAGS_N; ++n) { float* c_ptr = c_ptr_base + (m * WARP_M) * N + (n * WARP_N); // LDM = N wmma::store_matrix_sync(c_ptr, c_frag[m][n], N, wmma::mem_row_major); } } } Very interesting facts: 64 x 64 x 64 is the same as 128 * 32 * 128, but we can get a larger C blocks in the second strategy, which is 128 x 128 and this give us more benefits, we prefer to use panel size of tiled matrix to maxmiaze the compututation under a certain amount of bandwidth.\nWhat could be done next? As for now, you should have a good understanding of the CUDA and GPU meomry hierachy, but we could still optimize it with async data fetching and double buffering(latency hiding). I will cover it in the later articles.\n",
  "wordCount" : "3835",
  "inLanguage": "en",
  "datePublished": "2025-11-16T00:06:00Z",
  "dateModified": "2025-11-16T00:06:00Z",
  "author":{
    "@type": "Person",
    "name": "Xi Chen"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://xichen1997.github.io/posts/2025-11-17-introduction-to-cuda-and-high-performance-cuda-kernels/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Xi's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://xichen1997.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://xichen1997.github.io/" accesskey="h" title="Xi&#39;s Blog (Alt + H)">Xi&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://xichen1997.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://xichen1997.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://xichen1997.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://xichen1997.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://xichen1997.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Kernel comparison with a MMA in CUDA and near-SOTA/cuBLAS performance kernel
    </h1>
    <div class="post-meta"><span title='2025-11-16 00:06:00 +0000 +0000'>November 16, 2025</span>&nbsp;·&nbsp;<span>19 min</span>&nbsp;·&nbsp;<span>3835 words</span>&nbsp;·&nbsp;<span>Xi Chen</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#setup-before-doing-the-matrix-computation">Setup before doing the matrix computation.</a></li>
    <li><a href="#naive-kernel---write-first-kernel-function-in-cuda">Naive kernel - write first kernel function in CUDA</a>
      <ul>
        <li><a href="#tips-the-cuda-hardware-and-memory-hierachy-use-rtx-4090ada-arch-as-an-example">Tips: the CUDA hardware and memory hierachy, use RTX 4090(ada arch) as an example</a></li>
        <li><a href="#back-to-the-naive-kernel">Back to the naive kernel</a></li>
      </ul>
    </li>
    <li><a href="#can-we-make-it-better">Can we make it better?</a>
      <ul>
        <li><a href="#analysis-of-memory-access-pattern">analysis of memory access pattern</a></li>
        <li><a href="#try-1d-access-pattern">Try 1D access pattern</a></li>
      </ul>
    </li>
    <li><a href="#tiled-matrix-improve-the-data-transfer-efficiency-to-boost-computation">Tiled-matrix: Improve the data transfer efficiency to boost computation.</a></li>
    <li><a href="#warp-level-matrix-multiplication---the-secrets-of-new-gpu-architecture">Warp level matrix multiplication - the secrets of new GPU architecture</a></li>
    <li><a href="#final-steps---combine-warp-level-matrix-multiplication-and-shared-memory">Final steps - combine warp level matrix multiplication and shared memory</a></li>
    <li><a href="#what-could-be-done-next">What could be done next?</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="kernel-comparison-with-a-mma-in-cuda-and-near-sotacublas-performance-kernel">Kernel comparison with a MMA in CUDA and near-SOTA/cuBLAS performance kernel<a hidden class="anchor" aria-hidden="true" href="#kernel-comparison-with-a-mma-in-cuda-and-near-sotacublas-performance-kernel">#</a></h1>
<p>The project is hosted in the repository:
<a href="https://github.com/xichen1997/CUDA-refresh">CUDA-refresh</a></p>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>The kernel is the &ldquo;kernel&rdquo; in the concept of CUDA, it directly influence the compute efficiency and it&rsquo;s the key to take advanage of GPU&rsquo;s huge amount of computation resource and bandwidth.</p>
<p>Here is a simple refresh of the CUDA calculation and memory hierachy and their infleunce to the computation efficiency.</p>
<p>In the tutorial, several concepts will be compare like $threads | blocks$ numbers, warp, shared memory, and double buffering(not finished yet).</p>
<h2 id="setup-before-doing-the-matrix-computation">Setup before doing the matrix computation.<a hidden class="anchor" aria-hidden="true" href="#setup-before-doing-the-matrix-computation">#</a></h2>
<p>In this section, multiple benchmark will be applied on different kernels like the performance, including kernel Size, matrix size and the way how we use memory(shared memory or L2 cache, etc)</p>
<p>We only focus on the kernel of MMA so the main functions are almost the same in each kernels&rsquo; benchmark.</p>
<p>You can check the <code>main.cu</code> in each steps folder to check how the benchmark is run. They should be pretty straight forward. And if you want to test, make sure you have an NVIDIA GPU(The current compilation only support RTX 30 series and 40 series, if you want backward compatibility please change the compliation switch <code>-arch=sm89</code> to previous version.)</p>
<p><code>benchmark.sh</code> will be used to measure the time spent on creating matrix and cuda computation seperately by timing. You can run <code>make benchmark</code> to get the logs/ folder and check its running result with different size of matrix.</p>
<p>In the heavy profiling stage we will use <code>ncu</code> to do the benchmark, it can show us how much % we use for the GPU&rsquo;s SM units, please refer to the <a href="https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html">documentation</a> at Nvidia. Call it with <code>make benchmark-heavy</code></p>
<h2 id="naive-kernel---write-first-kernel-function-in-cuda">Naive kernel - write first kernel function in CUDA<a hidden class="anchor" aria-hidden="true" href="#naive-kernel---write-first-kernel-function-in-cuda">#</a></h2>
<p>In this section, the MMA(matrix multiplication) is implemented in a very simple way. Compared to write in CPU, in GPU, we parallelize the first two loops and each thread calculate the result for a single element in C.
$$C[i][j] = \sum_k A[i][k] * B[k][j]$$</p>
<p>It&rsquo;s easy to write the kernel: for each thread, we calculate the row and col accoding to the threadIdx.x and threadIdx.y.</p>
<p>Note: The size of the blockDim.x * blockDim.y * blockDim.z &lt;= 2048(phsical threads limitation). But the Grid size can be set to a large number like 64k(It&rsquo;s just a logic dimension to distribute different tasks to different threads).</p>
<p>Each threads or warp(a group of threads which will be covered later) will gover a certain range of the problems which can be parallel, like in the matrix multiplication, naive kernel each thread will be responsible for calculate one element in C use <code>A[i][:] * B[:][j]</code>.</p>
<p>In the <code>main.cu</code> file in step-1, we can use the matrix dimension to decide the grid size:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl">    <span class="c1">// make sure one block is multiple of 32 for better performance, 32 threads is in a warp
</span></span></span><span class="line"><span class="cl">    <span class="n">dim3</span> <span class="nf">blockSize</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">dim3</span> <span class="nf">gridSize</span><span class="p">((</span><span class="n">N</span> <span class="o">+</span> <span class="n">blockSize</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">blockSize</span><span class="p">.</span><span class="n">x</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                  <span class="p">(</span><span class="n">M</span> <span class="o">+</span> <span class="n">blockSize</span><span class="p">.</span><span class="n">y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">blockSize</span><span class="p">.</span><span class="n">y</span><span class="p">);</span> <span class="c1">// always generate more than enough to prevent the task scale is not enough to cover the C matrix scale.
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>In this place I use 32x32 = 1024 threads in each block.</p>
<h3 id="tips-the-cuda-hardware-and-memory-hierachy-use-rtx-4090ada-arch-as-an-example">Tips: the CUDA hardware and memory hierachy, use RTX 4090(ada arch) as an example<a hidden class="anchor" aria-hidden="true" href="#tips-the-cuda-hardware-and-memory-hierachy-use-rtx-4090ada-arch-as-an-example">#</a></h3>
<p>GPU has more parallel computing capacity compared to CPU. In CUDA, the calculation style is SIMT(single instructions multiple threads), compared to SIMD(single instruction multiple data), each threads has their own registers.</p>
<p>And 32 threads are grouped together to execute the same instruction together, so the minimal schedule unit is warp not thread in the GPU.</p>
<p>The warps/threads can be logically grouped into a block, the threads number must be a multiple of 32. And SM(Streaming Multiprocessor) can execute the block, one SM has certain numbers of hardware like shared memory and registers, which will decide how many blocks can run on a SM. (note 1 block can only be a resident in 1 SM)</p>
<p>For example, Ada Lovelace GPU(RTX 4090) has 128 SMs. Each SM has 64k registers and 100KB shared memory(shared with L1 cache), 2048 threads. That&rsquo;s the phsical limitation. Note, L2 cache is shared by all SMs, not exclusive to one SM.</p>
<p>And each SM has 4 warp schedulers, which means it can issue 4 instruction to warps at the same cycle. Thus, block at least should have 4 warps to maxmize the performance of each SM.</p>
<p>The GPU is connected to HBM(24GB for RTX 4090) which bandwidth reaches 1008GB/s. That&rsquo;s the phsical limitation of the data flow and decide how many calculation we can do.</p>
<p>Then we should take a look into how the block size and grid size to match the matrix size:
In CUDA, for the row we calculate use <code>row = threadIdx.y + blockIdx.y * blockDim.y</code> and <code>col = threadIdx.x + blockIdx.x * blockDim.x</code> if we use 2D representation because that&rsquo;s the rules in CUDA. That&rsquo;s might be unintuitive at the beginning but you can think in this way: the thread should be continous along x then y, so in the matrix, the matrix is row-major, so the changing of x reflect the change of col. They are trying to improve cache locality.</p>
<h3 id="back-to-the-naive-kernel">Back to the naive kernel<a hidden class="anchor" aria-hidden="true" href="#back-to-the-naive-kernel">#</a></h3>
<p>After introduce the memory layout and rules it should be easy to write the kernel:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">kernel1</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="k">const</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">K</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span><span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">M</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">){</span> <span class="c1">// prevent over the boundary
</span></span></span><span class="line"><span class="cl">        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">            <span class="n">sum</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">C</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>After running the <code>make benchmark</code> we can check the result in the log, for the 8192 * 8192 matrix size, the computational timing and allocation timings are:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">Matrix size: 8192x8192 * 8192x8192
</span></span><span class="line"><span class="cl">------------------time used ----------------------
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Initialization time: 0.457656 seconds
</span></span><span class="line"><span class="cl">Computation time: 0.256279 seconds
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">--------------------------------------------------
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="can-we-make-it-better">Can we make it better?<a hidden class="anchor" aria-hidden="true" href="#can-we-make-it-better">#</a></h2>
<p>The answer is yes, recall how the array is stored in C++. It&rsquo;s row major, which means the memory address is continuous for each row.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">arr</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">}</span> <span class="p">,{</span> <span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">}</span> <span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="c1">// a[0][0] a[0][1] a[0][2] a[0][3] they are continuous.
</span></span></span><span class="line"><span class="cl"><span class="c1">// &amp;a[1][0] - &amp;a[0][0] = 4(stride)
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>So why don&rsquo;t we store B in col-major? It means we still take the logic <code>B[row][col]</code> but they are not align with their phsical location. Assuming the B is KxN matrix, then the <code>B[row][col] = B'[col * K + row]</code> in 1D.</p>
<p>The experienment can be done quickly in step-2, we change how we allocate memory to the matrix B(transpose it before do cuda memory copy):</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl">    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_B</span><span class="p">[</span><span class="n">j</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">j</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>remember we transpose B already so $B[j][i] = B^T[i][j] = B&rsquo;[i*K+j]$
$$ C_{ij} = \sum_k A_{ik} * B_{kj} = \sum_k A[row * K + i] * B[col * K + j] $$</p>
<p>The kernel is:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">kernel2</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="k">const</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">K</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span><span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">M</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">            <span class="n">sum</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">col</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">C</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>That&rsquo;s should work, let&rsquo;s see the result for the 8192 dimension compare to naive kernel:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">Matrix size: 8192x8192 * 8192x8192
</span></span><span class="line"><span class="cl">------------------time used ----------------------
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Initialization time: 0.554599 seconds
</span></span><span class="line"><span class="cl">Computation time: 1.65866 seconds
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">--------------------------------------------------
</span></span></code></pre></td></tr></table>
</div>
</div><p>Wait?!  Why it needs more time than naive kernel???</p>
<h3 id="analysis-of-memory-access-pattern">analysis of memory access pattern<a hidden class="anchor" aria-hidden="true" href="#analysis-of-memory-access-pattern">#</a></h3>
<p>We know the cache locality will define the calculation, why the pattern fail in this case?</p>
<p>The answer is we are using a lot of different threads to calculate it, each threads represents a different row, col. Inside a warp, all the threads ids are continuous, they are accessing the same row and different cols.</p>
<p>And inside a kernel, at the same time they are accessing the data: <code>sum += A[row * K + i] * B[col * K + i];</code>, different threads will use different col, and their stride is K which is 8192 and this cause all the threads could reuse data another threads access.</p>
<p>Warp has a feature called data coelesing, which means if threads inside a warp are accessing the continuous memory, they can combine the requests into a single one.</p>
<p>That explains why the performance dropped a lot, it improves the single threads efficiency but require too much bandwidth.</p>
<h3 id="try-1d-access-pattern">Try 1D access pattern<a hidden class="anchor" aria-hidden="true" href="#try-1d-access-pattern">#</a></h3>
<p>Open step-4 to do same experiment with step-1 but represent the matrix in 1D, you will find the performance improve a little bit, but not too much</p>
<h2 id="tiled-matrix-improve-the-data-transfer-efficiency-to-boost-computation">Tiled-matrix: Improve the data transfer efficiency to boost computation.<a hidden class="anchor" aria-hidden="true" href="#tiled-matrix-improve-the-data-transfer-efficiency-to-boost-computation">#</a></h2>
<p>After we know the properties of warp and block(all the threads in a block can use the same shared_memory), we can conside data reuse.</p>
<p>Recall what we have done in the naive kernel:
$$C[i][j] = \sum_k A[i][k] * B[k][j]$$</p>
<p>each thread use their own memory, which requires a row of A and a col of B, which is a huge waste. What can we do better? Tiled memory calculate, assuming we are taking a band of 16 x K of A and K x 16 of B to calculate the 16x16 of C_blocks matrix.</p>
<p>Of course we are using block matrix multiplication, we can&rsquo;t store the two bands of matrix together but when we iterate over K dimension, assuming the stride is 16, then we need a loop for K / 16 times, and each time we put the 16x16 A and 16x16 B tiles into shared memory to do mma add to C block(16x16).</p>
<p>How many times we can reuse it? 16 * 16 * 16 (calculation FLOPS) / 2 * 16 * 16 (the A tile and B tile) = 8 times, that&rsquo;s a huge boost, we can extend the number to 32, 64, etc, with larger tile. However, the tile can not expand infinitely because of the limitation of 100KB limitation(SRAM is very expensive!).</p>
<p>The best size in RTX 4090 is about 128x128 or 256 * 64, you can run and compare the step-6 and step-5 for comparison, of course in 5 the block size is 16 x 16 and it cause extra overhead to move the data into the shared memory and that offset the benefit of data reuse.</p>
<p>Here is the kernel for 32 x 32 C block size, you can try different block size and benchmark thems.</p>
<p>Notice the loading process is complete by all the threads in the blocks. You can use the size of A or B blocks / threads number to decide which thread load a certain or several elements of a matrix.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Kernel function to do naive matrix multiplication
</span></span></span><span class="line"><span class="cl"><span class="c1">// A[i][j] = i * N + j;
</span></span></span><span class="line"><span class="cl"><span class="c1">// B[i][j] = i * K + j;
</span></span></span><span class="line"><span class="cl"><span class="cp"># define TILE 32
</span></span></span><span class="line"><span class="cl"><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">kernel_tile</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="k">const</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">K</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// inside of a tile, local index
</span></span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">tx</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">ty</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// block index
</span></span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">bx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">by</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// global row and col index
</span></span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">by</span> <span class="o">*</span> <span class="n">TILE</span> <span class="o">+</span> <span class="n">ty</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">bx</span> <span class="o">*</span> <span class="n">TILE</span> <span class="o">+</span> <span class="n">tx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">As</span><span class="p">[</span><span class="n">TILE</span><span class="p">][</span><span class="n">TILE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">Bs</span><span class="p">[</span><span class="n">TILE</span><span class="p">][</span><span class="n">TILE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// loop over all tiles
</span></span></span><span class="line"><span class="cl">    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">K</span> <span class="o">+</span> <span class="n">TILE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">TILE</span><span class="p">;</span> <span class="o">++</span><span class="n">t</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// load A and B tiles into shared memory
</span></span></span><span class="line"><span class="cl">        <span class="c1">// each thread loads one element of each tile
</span></span></span><span class="line"><span class="cl">        <span class="k">if</span><span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">M</span> <span class="o">&amp;&amp;</span> <span class="n">t</span> <span class="o">*</span> <span class="n">TILE</span> <span class="o">+</span> <span class="n">tx</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">            <span class="n">As</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">t</span> <span class="o">*</span> <span class="n">TILE</span> <span class="o">+</span> <span class="n">tx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span><span class="k">else</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">As</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span><span class="p">(</span><span class="n">col</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">&amp;&amp;</span> <span class="n">t</span> <span class="o">*</span> <span class="n">TILE</span> <span class="o">+</span> <span class="n">ty</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">            <span class="n">Bs</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[(</span><span class="n">t</span> <span class="o">*</span> <span class="n">TILE</span> <span class="o">+</span> <span class="n">ty</span><span class="p">)</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span>  <span class="n">col</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span><span class="k">else</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">Bs</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// compute partial sum for this tile
</span></span></span><span class="line"><span class="cl">        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">TILE</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">            <span class="n">sum</span> <span class="o">+=</span> <span class="n">As</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">Bs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">tx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// write back the result
</span></span></span><span class="line"><span class="cl">    <span class="k">if</span><span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">M</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">        <span class="n">C</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="warp-level-matrix-multiplication---the-secrets-of-new-gpu-architecture">Warp level matrix multiplication - the secrets of new GPU architecture<a hidden class="anchor" aria-hidden="true" href="#warp-level-matrix-multiplication---the-secrets-of-new-gpu-architecture">#</a></h2>
<p>Currently the CUDA cores (which are the normal ALU elements) in the SM can do a lot of operations, but 90% of calculation in deep learning are matrix multiplication. Thus, a new warp-level of hardware is introduced, tensor core, we need use warp to schedule it.</p>
<p>Within a cycle, normal warp can do 64 FLOPS, but tensor core can do 512 FLOPS for a 16 x 16 matrix. (which is 8x boost.)</p>
<p>To use the kernel, we need to define the fragments(16x16) of A, B and C:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl">    <span class="c1">// define tensor core fragment
</span></span></span><span class="line"><span class="cl">    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_a</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">half</span><span class="p">,</span> <span class="n">wmma</span><span class="o">::</span><span class="n">row_major</span><span class="o">&gt;</span> <span class="n">a_frag</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_b</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">half</span><span class="p">,</span> <span class="n">wmma</span><span class="o">::</span><span class="n">row_major</span><span class="o">&gt;</span> <span class="n">b_frag</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">accumulator</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="kt">float</span><span class="o">&gt;</span> <span class="n">c_frag</span><span class="p">;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>F
At this stage, we should divide a sub sub matrix in C for the warp to control, for simplicity, we assume the block size is 16 x 16 and the warp control all the block, then we can have the kernel to boost the 16 x 16 tiled matrix multiplication by:</p>
<p>$$C_sub = A[16][:] @ B[:][16] = A[16][0..15] @ [0..15][16] +  A[16][16..31] @ [16..31][16] + &hellip;$$</p>
<p>Here is the kernel:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;mma.h&gt;</span><span class="cp"> </span><span class="c1">// for wmma
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;cuda_fp16.h&gt;</span><span class="cp"> </span><span class="c1">// for half
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="k">namespace</span> <span class="n">nvcuda</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Kernel function to do naive matrix multiplication
</span></span></span><span class="line"><span class="cl"><span class="c1">// A[i][j] = i * N + j;
</span></span></span><span class="line"><span class="cl"><span class="c1">// B[i][j] = i * K + j;
</span></span></span><span class="line"><span class="cl"><span class="cp"># define TILE 16
</span></span></span><span class="line"><span class="cl"><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">kernel_tile</span><span class="p">(</span><span class="k">const</span> <span class="n">half</span> <span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="k">const</span> <span class="n">half</span> <span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="k">const</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">K</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1">// define tensor core fragment
</span></span></span><span class="line"><span class="cl">    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_a</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">half</span><span class="p">,</span> <span class="n">wmma</span><span class="o">::</span><span class="n">row_major</span><span class="o">&gt;</span> <span class="n">a_frag</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_b</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">half</span><span class="p">,</span> <span class="n">wmma</span><span class="o">::</span><span class="n">row_major</span><span class="o">&gt;</span> <span class="n">b_frag</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">accumulator</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="kt">float</span><span class="o">&gt;</span> <span class="n">c_frag</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// initialization of C fragment
</span></span></span><span class="line"><span class="cl">    <span class="n">wmma</span><span class="o">::</span><span class="n">fill_fragment</span><span class="p">(</span><span class="n">c_frag</span><span class="p">,</span> <span class="mf">0.0f</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">block_row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">block_col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="n">half</span> <span class="o">*</span><span class="n">A_ptr</span> <span class="o">=</span> <span class="n">A</span> <span class="o">+</span> <span class="n">block_row</span> <span class="o">*</span> <span class="n">TILE</span> <span class="o">*</span> <span class="n">K</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="n">half</span> <span class="o">*</span><span class="n">B_ptr</span> <span class="o">=</span> <span class="n">B</span> <span class="o">+</span> <span class="n">block_col</span> <span class="o">*</span> <span class="n">TILE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">TILE</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// load the intput matrices A and B to fragments
</span></span></span><span class="line"><span class="cl">        <span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">a_frag</span><span class="p">,</span> <span class="n">A_ptr</span><span class="p">,</span> <span class="n">K</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">b_frag</span><span class="p">,</span> <span class="n">B_ptr</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// perform the matrix multiplication
</span></span></span><span class="line"><span class="cl">        <span class="n">wmma</span><span class="o">::</span><span class="n">mma_sync</span><span class="p">(</span><span class="n">c_frag</span><span class="p">,</span> <span class="n">a_frag</span><span class="p">,</span> <span class="n">b_frag</span><span class="p">,</span> <span class="n">c_frag</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// calculate the starting row and column index of C matrix
</span></span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="o">*</span> <span class="n">c_ptr</span> <span class="o">=</span> <span class="n">C</span> <span class="o">+</span> <span class="n">block_row</span> <span class="o">*</span> <span class="n">TILE</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">block_col</span> <span class="o">*</span> <span class="n">TILE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// write memory back to C
</span></span></span><span class="line"><span class="cl">    <span class="c1">// fix the error: no matching function for call to &#39;store_matrix_sync&#39;
</span></span></span><span class="line"><span class="cl">    <span class="n">wmma</span><span class="o">::</span><span class="n">store_matrix_sync</span><span class="p">(</span><span class="n">c_ptr</span><span class="p">,</span> <span class="n">c_frag</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">wmma</span><span class="o">::</span><span class="n">mem_row_major</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="final-steps---combine-warp-level-matrix-multiplication-and-shared-memory">Final steps - combine warp level matrix multiplication and shared memory<a hidden class="anchor" aria-hidden="true" href="#final-steps---combine-warp-level-matrix-multiplication-and-shared-memory">#</a></h2>
<p>In this way, we know how to use data better to boost the bandwidth. Then the wmma can be used with shared memory, in this case we use 128 x 128 x 32 kernel (A tile is 128 x 32, B tile is 32 x 128 and C tile is 128 x 128 to save shared memory). The shared memory usage is 128 * 32 * 2 = 8192 elements and 16KB usage(for half data type is 2 Bytes) which is pretty good, it will not pressure L1 cache a lot and can hold 2-4 blocks in a SM.</p>
<p>Here is the kernel, notice each block control 128 x 128 matrix, and 16 warps will consumes this. Each warp get 32 x 32 warp level tile matrix, it should be divided into 4 fragments matrix and do the calculation. Here is the kernel, you can also try it in step-12.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;mma.h&gt;</span><span class="cp"> </span><span class="c1">// for wmma
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;cuda_fp16.h&gt;</span><span class="cp"> </span><span class="c1">// for half
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="k">namespace</span> <span class="n">nvcuda</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// warp size wichi is constant 16 x 16 x 16
</span></span></span><span class="line"><span class="cl"><span class="cp">#define WARP_M 16
</span></span></span><span class="line"><span class="cl"><span class="cp">#define WARP_N 16
</span></span></span><span class="line"><span class="cl"><span class="cp">#define WARP_K 16
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// block tile size
</span></span></span><span class="line"><span class="cl"><span class="cp">#define BLOCK_M 128 </span><span class="c1">// 8 * WARP_M
</span></span></span><span class="line"><span class="cl"><span class="cp">#define BLOCK_N 128 </span><span class="c1">// 8 * WARP_N
</span></span></span><span class="line"><span class="cl"><span class="cp">#define BLOCK_K 32  </span><span class="c1">// 8 * WARP_K 
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// 
</span></span></span><span class="line"><span class="cl"><span class="cp">#define WARPS_PER_BLOCK_M 4 </span><span class="c1">// block number of warps in M dimension
</span></span></span><span class="line"><span class="cl"><span class="cp">#define WARPS_PER_BLOCK_N 4 </span><span class="c1">// block number of warps in N dimension
</span></span></span><span class="line"><span class="cl"><span class="cp">#define BLOCK_WARPS (WARPS_PER_BLOCK_M * WARPS_PER_BLOCK_N) </span><span class="c1">// 4 * 4 = 16 warps per block
</span></span></span><span class="line"><span class="cl"><span class="cp">#define BLOCK_THREADS (32 * BLOCK_WARPS) </span><span class="c1">// 32 * 16 = 512 threads per block
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// the govern region of C_blocks for each warp
</span></span></span><span class="line"><span class="cl"><span class="cp">#define WARP_C_TILE_M (BLOCK_M / WARPS_PER_BLOCK_M) </span><span class="c1">// 128 / 4 = 32
</span></span></span><span class="line"><span class="cl"><span class="cp">#define WARP_C_TILE_N (BLOCK_N / WARPS_PER_BLOCK_N) </span><span class="c1">// 128 / 4 = 32
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// number of fragments of C for each warp
</span></span></span><span class="line"><span class="cl"><span class="cp">#define WARP_C_FRAGS_M (WARP_C_TILE_M / WARP_M) </span><span class="c1">// 32 / 16 = 2
</span></span></span><span class="line"><span class="cl"><span class="cp">#define WARP_C_FRAGS_N (WARP_C_TILE_N / WARP_N) </span><span class="c1">// 32 / 16 = 2
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">kernel_smem_tile_128x128</span><span class="p">(</span><span class="k">const</span> <span class="n">half</span> <span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="k">const</span> <span class="n">half</span> <span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                         <span class="k">const</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">K</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// A: (M, K) row-major
</span></span></span><span class="line"><span class="cl">    <span class="c1">// B: (K, N) col-major
</span></span></span><span class="line"><span class="cl">    <span class="c1">// C: (M, N) row-major
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// shared memory size for A and B tiles 4096 + 4,096 = 8,192 elements
</span></span></span><span class="line"><span class="cl">    <span class="c1">// 16KB in total.
</span></span></span><span class="line"><span class="cl">    <span class="c1">// A tile: (BLOCK_M, BLOCK_K) -&gt; (128, 32) = 4096
</span></span></span><span class="line"><span class="cl">    <span class="n">__shared__</span> <span class="n">half</span> <span class="n">smem_a</span><span class="p">[</span><span class="n">BLOCK_M</span> <span class="o">*</span> <span class="n">BLOCK_K</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// B tile: (BLOCK_K, BLOCK_N) -&gt; (32, 128) = 4096
</span></span></span><span class="line"><span class="cl">    <span class="n">__shared__</span> <span class="n">half</span> <span class="n">smem_b</span><span class="p">[</span><span class="n">BLOCK_K</span> <span class="o">*</span> <span class="n">BLOCK_N</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1">// fragments of A and B
</span></span></span><span class="line"><span class="cl">    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_a</span><span class="p">,</span> <span class="n">WARP_M</span><span class="p">,</span> <span class="n">WARP_N</span><span class="p">,</span> <span class="n">WARP_K</span><span class="p">,</span> <span class="n">half</span><span class="p">,</span> <span class="n">wmma</span><span class="o">::</span><span class="n">row_major</span><span class="o">&gt;</span> <span class="n">a_frag</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_b</span><span class="p">,</span> <span class="n">WARP_M</span><span class="p">,</span> <span class="n">WARP_N</span><span class="p">,</span> <span class="n">WARP_K</span><span class="p">,</span> <span class="n">half</span><span class="p">,</span> <span class="n">wmma</span><span class="o">::</span><span class="n">col_major</span><span class="o">&gt;</span> <span class="n">b_frag</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// fragmesnts of C (in total 8 fragments per warp)
</span></span></span><span class="line"><span class="cl">    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">accumulator</span><span class="p">,</span> <span class="n">WARP_M</span><span class="p">,</span> <span class="n">WARP_N</span><span class="p">,</span> <span class="n">WARP_K</span><span class="p">,</span> <span class="kt">float</span><span class="o">&gt;</span> <span class="n">c_frag</span><span class="p">[</span><span class="n">WARP_C_FRAGS_M</span><span class="p">][</span><span class="n">WARP_C_FRAGS_N</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// initialization of C fragments in each warp
</span></span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">m</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">m</span> <span class="o">&lt;</span> <span class="n">WARP_C_FRAGS_M</span><span class="p">;</span> <span class="o">++</span><span class="n">m</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="n">WARP_C_FRAGS_N</span><span class="p">;</span> <span class="o">++</span><span class="n">n</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">wmma</span><span class="o">::</span><span class="n">fill_fragment</span><span class="p">(</span><span class="n">c_frag</span><span class="p">[</span><span class="n">m</span><span class="p">][</span><span class="n">n</span><span class="p">],</span> <span class="mf">0.0f</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 4 x 4 warps per block
</span></span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">warpId</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span> <span class="c1">// 0..15
</span></span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">laneId</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="c1">// 0..31
</span></span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">block_row_base</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">BLOCK_M</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">block_col_base</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">BLOCK_N</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1">// warpId 0, 32, 64 
</span></span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">warp_row_offset</span> <span class="o">=</span> <span class="p">(</span><span class="n">warpId</span> <span class="o">/</span> <span class="n">WARPS_PER_BLOCK_N</span><span class="p">)</span> <span class="o">*</span> <span class="n">WARP_C_TILE_M</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">    <span class="c1">// 0, 32, 64 
</span></span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">warp_col_offset</span> <span class="o">=</span> <span class="p">(</span><span class="n">warpId</span> <span class="o">%</span> <span class="n">WARPS_PER_BLOCK_N</span><span class="p">)</span> <span class="o">*</span> <span class="n">WARP_C_TILE_N</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// thread ID within the block for loading shared memory
</span></span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="c1">// 0..511
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k_tile_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k_tile_idx</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">k_tile_idx</span> <span class="o">+=</span> <span class="n">BLOCK_K</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">const</span> <span class="n">half</span> <span class="o">*</span><span class="n">A_gmem_ptr</span> <span class="o">=</span> <span class="n">A</span> <span class="o">+</span> <span class="n">block_row_base</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">k_tile_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">const</span> <span class="n">half</span> <span class="o">*</span><span class="n">B_gmem_ptr</span> <span class="o">=</span> <span class="n">B</span> <span class="o">+</span> <span class="n">block_col_base</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">k_tile_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// A (row-major) load
</span></span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">BLOCK_M</span> <span class="o">*</span> <span class="n">BLOCK_K</span> <span class="o">/</span> <span class="n">BLOCK_THREADS</span><span class="p">);</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// 4096 / 512 = 8
</span></span></span><span class="line"><span class="cl">            <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">tid</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">BLOCK_THREADS</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">/</span> <span class="n">BLOCK_K</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">%</span> <span class="n">BLOCK_K</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">smem_a</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">A_gmem_ptr</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// B (col-major) load
</span></span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">BLOCK_K</span> <span class="o">*</span> <span class="n">BLOCK_N</span> <span class="o">/</span> <span class="n">BLOCK_THREADS</span><span class="p">);</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// 4096 / 512 = 8
</span></span></span><span class="line"><span class="cl">            <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">tid</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">BLOCK_THREADS</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">/</span> <span class="n">BLOCK_N</span><span class="p">;</span> <span class="c1">// (k-dim)
</span></span></span><span class="line"><span class="cl">            <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">%</span> <span class="n">BLOCK_N</span><span class="p">;</span> <span class="c1">// (n-dim)
</span></span></span><span class="line"><span class="cl">            <span class="n">smem_b</span><span class="p">[</span><span class="n">col</span> <span class="o">*</span> <span class="n">BLOCK_K</span> <span class="o">+</span> <span class="n">row</span><span class="p">]</span> <span class="o">=</span> <span class="n">B_gmem_ptr</span><span class="p">[</span><span class="n">col</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">row</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1">// K-loop (inner): BLOCK_K (32) / WARP_K (16) = 2 
</span></span></span><span class="line"><span class="cl">        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">inner_k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">inner_k</span> <span class="o">&lt;</span> <span class="n">BLOCK_K</span><span class="p">;</span> <span class="n">inner_k</span> <span class="o">+=</span> <span class="n">WARP_K</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1">// calculate all fragments of A(16 x 32) and B(32 x 16) for this warp to compute a C fragment(16x16)
</span></span></span><span class="line"><span class="cl">            <span class="c1">// so there are total 2 times of wmma iuoperations per 
</span></span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">m</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">m</span> <span class="o">&lt;</span> <span class="n">WARP_C_FRAGS_M</span><span class="p">;</span> <span class="o">++</span><span class="n">m</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// 0..
</span></span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="n">WARP_C_FRAGS_N</span><span class="p">;</span> <span class="o">++</span><span class="n">n</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// 0..1
</span></span></span><span class="line"><span class="cl">                    
</span></span><span class="line"><span class="cl">                    <span class="c1">// A tile for this warp starts at smem_a[warp_row_offset * BLOCK_K]
</span></span></span><span class="line"><span class="cl">                    <span class="c1">// M-dim offset: m * WARP_M
</span></span></span><span class="line"><span class="cl">                    <span class="k">const</span> <span class="n">half</span> <span class="o">*</span> <span class="n">A_smem_ptr</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">smem_a</span><span class="p">[</span> <span class="p">(</span><span class="n">warp_row_offset</span> <span class="o">+</span> <span class="n">m</span> <span class="o">*</span> <span class="n">WARP_M</span><span class="p">)</span> <span class="o">*</span> <span class="n">BLOCK_K</span> <span class="o">+</span> <span class="n">inner_k</span> <span class="p">];</span>
</span></span><span class="line"><span class="cl">                    
</span></span><span class="line"><span class="cl">                    <span class="c1">// B tile for this warp starts at smem_b[warp_col_offset * BLOCK_K]
</span></span></span><span class="line"><span class="cl">                    <span class="c1">// N-dim offset: n * WARP_N
</span></span></span><span class="line"><span class="cl">                    <span class="k">const</span> <span class="n">half</span> <span class="o">*</span> <span class="n">B_smem_ptr</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">smem_b</span><span class="p">[</span> <span class="p">(</span><span class="n">warp_col_offset</span> <span class="o">+</span> <span class="n">n</span> <span class="o">*</span> <span class="n">WARP_N</span><span class="p">)</span> <span class="o">*</span> <span class="n">BLOCK_K</span> <span class="o">+</span> <span class="n">inner_k</span> <span class="p">];</span>
</span></span><span class="line"><span class="cl">                    
</span></span><span class="line"><span class="cl">                    <span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">a_frag</span><span class="p">,</span> <span class="n">A_smem_ptr</span><span class="p">,</span> <span class="n">BLOCK_K</span><span class="p">);</span> <span class="c1">// LDM=BLOCK_K (32)
</span></span></span><span class="line"><span class="cl">                    <span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">b_frag</span><span class="p">,</span> <span class="n">B_smem_ptr</span><span class="p">,</span> <span class="n">BLOCK_K</span><span class="p">);</span> <span class="c1">// LDM=BLOCK_K (32)
</span></span></span><span class="line"><span class="cl">                    
</span></span><span class="line"><span class="cl">                    <span class="c1">// accumulate
</span></span></span><span class="line"><span class="cl">                    <span class="n">wmma</span><span class="o">::</span><span class="n">mma_sync</span><span class="p">(</span><span class="n">c_frag</span><span class="p">[</span><span class="n">m</span><span class="p">][</span><span class="n">n</span><span class="p">],</span> <span class="n">a_frag</span><span class="p">,</span> <span class="n">b_frag</span><span class="p">,</span> <span class="n">c_frag</span><span class="p">[</span><span class="n">m</span><span class="p">][</span><span class="n">n</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">                <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">__syncthreads</span><span class="p">();</span> <span class="c1">// synchronize before loading the next tile
</span></span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// write back C fragments to global memory
</span></span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="o">*</span> <span class="n">c_ptr_base</span> <span class="o">=</span> <span class="n">C</span> <span class="o">+</span> <span class="p">(</span><span class="n">block_row_base</span> <span class="o">+</span> <span class="n">warp_row_offset</span><span class="p">)</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">block_col_base</span> <span class="o">+</span> <span class="n">warp_col_offset</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1">// write back all C fragments of this warp
</span></span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">m</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">m</span> <span class="o">&lt;</span> <span class="n">WARP_C_FRAGS_M</span><span class="p">;</span> <span class="o">++</span><span class="n">m</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="n">WARP_C_FRAGS_N</span><span class="p">;</span> <span class="o">++</span><span class="n">n</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="kt">float</span><span class="o">*</span> <span class="n">c_ptr</span> <span class="o">=</span> <span class="n">c_ptr_base</span> <span class="o">+</span> <span class="p">(</span><span class="n">m</span> <span class="o">*</span> <span class="n">WARP_M</span><span class="p">)</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">WARP_N</span><span class="p">);</span> <span class="c1">// LDM = N
</span></span></span><span class="line"><span class="cl">            <span class="n">wmma</span><span class="o">::</span><span class="n">store_matrix_sync</span><span class="p">(</span><span class="n">c_ptr</span><span class="p">,</span> <span class="n">c_frag</span><span class="p">[</span><span class="n">m</span><span class="p">][</span><span class="n">n</span><span class="p">],</span> <span class="n">N</span><span class="p">,</span> <span class="n">wmma</span><span class="o">::</span><span class="n">mem_row_major</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Very interesting facts: 64 x 64 x 64 is the same as 128 * 32 * 128, but we can get a larger C blocks in the second strategy, which is 128 x 128 and this give us more benefits, we prefer to use panel size of tiled matrix to maxmiaze the compututation under a certain amount of bandwidth.</p>
<h2 id="what-could-be-done-next">What could be done next?<a hidden class="anchor" aria-hidden="true" href="#what-could-be-done-next">#</a></h2>
<p>As for now, you should have a good understanding of the CUDA and GPU meomry hierachy, but we could still optimize it with async data fetching and double buffering(latency hiding). I will cover it in the later articles.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="next" href="https://xichen1997.github.io/posts/2025-09-19-add-array-support-for-dsl-based-on-minimal-cpu/">
    <span class="title">Next »</span>
    <br>
    <span>Add array support for DSL on Minimal CPU</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://xichen1997.github.io/">Xi&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
