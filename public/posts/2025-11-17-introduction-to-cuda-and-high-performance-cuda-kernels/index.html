<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="dark">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Kernel comparison with a MMA in CUDA and near-SOTA/cuBLAS performance kernel | Xi&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="Kernel comparison with a MMA in CUDA and near-SOTA/cuBLAS performance kernel
The project is hosted in the repository:
CUDA-refresh
Introduction
The kernel is the &ldquo;kernel&rdquo; in the concept of CUDA, it directly influence the compute efficiency and it&rsquo;s the key to take advanage of GPU&rsquo;s huge amount of computation resource and bandwidth.
Here is a simple refresh of the CUDA calculation and memory hierachy and their infleunce to the computation efficiency.">
<meta name="author" content="Xi Chen">
<link rel="canonical" href="http://localhost:1313/posts/2025-11-17-introduction-to-cuda-and-high-performance-cuda-kernels/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.56bf4be95d09faf55cd507c845718ccfcabb2e24e37f8fa5a66f9fa098252b06.css" integrity="sha256-Vr9L6V0J&#43;vVc1QfIRXGMz8q7LiTjf4&#43;lpm&#43;foJglKwY=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/2025-11-17-introduction-to-cuda-and-high-performance-cuda-kernels/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true
  },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
};

window.addEventListener('load', (event) => {
    document.querySelectorAll("mjx-container").forEach(function(x){
      x.parentElement.classList += 'has-jax'})
  });
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Xi&#39;s Blog (Alt + H)">Xi&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Kernel comparison with a MMA in CUDA and near-SOTA/cuBLAS performance kernel
    </h1>
    <div class="post-meta"><span title='2025-11-16 00:06:00 +0000 +0000'>November 16, 2025</span>&nbsp;·&nbsp;<span>19 min</span>&nbsp;·&nbsp;<span>3835 words</span>&nbsp;·&nbsp;<span>Xi Chen</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#setup-before-doing-the-matrix-computation">Setup before doing the matrix computation.</a></li>
    <li><a href="#naive-kernel---write-first-kernel-function-in-cuda">Naive kernel - write first kernel function in CUDA</a>
      <ul>
        <li><a href="#tips-the-cuda-hardware-and-memory-hierachy-use-rtx-4090ada-arch-as-an-example">Tips: the CUDA hardware and memory hierachy, use RTX 4090(ada arch) as an example</a></li>
        <li><a href="#back-to-the-naive-kernel">Back to the naive kernel</a></li>
      </ul>
    </li>
    <li><a href="#can-we-make-it-better">Can we make it better?</a>
      <ul>
        <li><a href="#analysis-of-memory-access-pattern">analysis of memory access pattern</a></li>
        <li><a href="#try-1d-access-pattern">Try 1D access pattern</a></li>
      </ul>
    </li>
    <li><a href="#tiled-matrix-improve-the-data-transfer-efficiency-to-boost-computation">Tiled-matrix: Improve the data transfer efficiency to boost computation.</a></li>
    <li><a href="#warp-level-matrix-multiplication---the-secrets-of-new-gpu-architecture">Warp level matrix multiplication - the secrets of new GPU architecture</a></li>
    <li><a href="#final-steps---combine-warp-level-matrix-multiplication-and-shared-memory">Final steps - combine warp level matrix multiplication and shared memory</a></li>
    <li><a href="#what-could-be-done-next">What could be done next?</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="kernel-comparison-with-a-mma-in-cuda-and-near-sotacublas-performance-kernel">Kernel comparison with a MMA in CUDA and near-SOTA/cuBLAS performance kernel<a hidden class="anchor" aria-hidden="true" href="#kernel-comparison-with-a-mma-in-cuda-and-near-sotacublas-performance-kernel">#</a></h1>
<p>The project is hosted in the repository:
<a href="https://github.com/xichen1997/CUDA-refresh">CUDA-refresh</a></p>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>The kernel is the &ldquo;kernel&rdquo; in the concept of CUDA, it directly influence the compute efficiency and it&rsquo;s the key to take advanage of GPU&rsquo;s huge amount of computation resource and bandwidth.</p>
<p>Here is a simple refresh of the CUDA calculation and memory hierachy and their infleunce to the computation efficiency.</p>
<p>In the tutorial, several concepts will be compare like $threads | blocks$ numbers, warp, shared memory, and double buffering(not finished yet).</p>
<h2 id="setup-before-doing-the-matrix-computation">Setup before doing the matrix computation.<a hidden class="anchor" aria-hidden="true" href="#setup-before-doing-the-matrix-computation">#</a></h2>
<p>In this section, multiple benchmark will be applied on different kernels like the performance, including kernel Size, matrix size and the way how we use memory(shared memory or L2 cache, etc)</p>
<p>We only focus on the kernel of MMA so the main functions are almost the same in each kernels&rsquo; benchmark.</p>
<p>You can check the <code>main.cu</code> in each steps folder to check how the benchmark is run. They should be pretty straight forward. And if you want to test, make sure you have an NVIDIA GPU(The current compilation only support RTX 30 series and 40 series, if you want backward compatibility please change the compliation switch <code>-arch=sm89</code> to previous version.)</p>
<p><code>benchmark.sh</code> will be used to measure the time spent on creating matrix and cuda computation seperately by timing. You can run <code>make benchmark</code> to get the logs/ folder and check its running result with different size of matrix.</p>
<p>In the heavy profiling stage we will use <code>ncu</code> to do the benchmark, it can show us how much % we use for the GPU&rsquo;s SM units, please refer to the <a href="https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html">documentation</a> at Nvidia. Call it with <code>make benchmark-heavy</code></p>
<h2 id="naive-kernel---write-first-kernel-function-in-cuda">Naive kernel - write first kernel function in CUDA<a hidden class="anchor" aria-hidden="true" href="#naive-kernel---write-first-kernel-function-in-cuda">#</a></h2>
<p>In this section, the MMA(matrix multiplication) is implemented in a very simple way. Compared to write in CPU, in GPU, we parallelize the first two loops and each thread calculate the result for a single element in C.
$$C[i][j] = \sum_k A[i][k] * B[k][j]$$</p>
<p>It&rsquo;s easy to write the kernel: for each thread, we calculate the row and col accoding to the threadIdx.x and threadIdx.y.</p>
<p>Note: The size of the blockDim.x * blockDim.y * blockDim.z &lt;= 2048(phsical threads limitation). But the Grid size can be set to a large number like 64k(It&rsquo;s just a logic dimension to distribute different tasks to different threads).</p>
<p>Each threads or warp(a group of threads which will be covered later) will gover a certain range of the problems which can be parallel, like in the matrix multiplication, naive kernel each thread will be responsible for calculate one element in C use <code>A[i][:] * B[:][j]</code>.</p>
<p>In the <code>main.cu</code> file in step-1, we can use the matrix dimension to decide the grid size:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl">    <span class="c1">// make sure one block is multiple of 32 for better performance, 32 threads is in a warp
</span></span></span><span class="line"><span class="cl">    <span class="n">dim3</span> <span class="nf">blockSize</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">dim3</span> <span class="nf">gridSize</span><span class="p">((</span><span class="n">N</span> <span class="o">+</span> <span class="n">blockSize</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">blockSize</span><span class="p">.</span><span class="n">x</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                  <span class="p">(</span><span class="n">M</span> <span class="o">+</span> <span class="n">blockSize</span><span class="p">.</span><span class="n">y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">blockSize</span><span class="p">.</span><span class="n">y</span><span class="p">);</span> <span class="c1">// always generate more than enough to prevent the task scale is not enough to cover the C matrix scale.
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>In this place I use 32x32 = 1024 threads in each block.</p>
<h3 id="tips-the-cuda-hardware-and-memory-hierachy-use-rtx-4090ada-arch-as-an-example">Tips: the CUDA hardware and memory hierachy, use RTX 4090(ada arch) as an example<a hidden class="anchor" aria-hidden="true" href="#tips-the-cuda-hardware-and-memory-hierachy-use-rtx-4090ada-arch-as-an-example">#</a></h3>
<p>GPU has more parallel computing capacity compared to CPU. In CUDA, the calculation style is SIMT(single instructions multiple threads), compared to SIMD(single instruction multiple data), each threads has their own registers.</p>
<p>And 32 threads are grouped together to execute the same instruction together, so the minimal schedule unit is warp not thread in the GPU.</p>
<p>The warps/threads can be logically grouped into a block, the threads number must be a multiple of 32. And SM(Streaming Multiprocessor) can execute the block, one SM has certain numbers of hardware like shared memory and registers, which will decide how many blocks can run on a SM. (note 1 block can only be a resident in 1 SM)</p>
<p>For example, Ada Lovelace GPU(RTX 4090) has 128 SMs. Each SM has 64k registers and 100KB shared memory(shared with L1 cache), 2048 threads. That&rsquo;s the phsical limitation. Note, L2 cache is shared by all SMs, not exclusive to one SM.</p>
<p>And each SM has 4 warp schedulers, which means it can issue 4 instruction to warps at the same cycle. Thus, block at least should have 4 warps to maxmize the performance of each SM.</p>
<p>The GPU is connected to HBM(24GB for RTX 4090) which bandwidth reaches 1008GB/s. That&rsquo;s the phsical limitation of the data flow and decide how many calculation we can do.</p>
<p>Then we should take a look into how the block size and grid size to match the matrix size:
In CUDA, for the row we calculate use <code>row = threadIdx.y + blockIdx.y * blockDim.y</code> and <code>col = threadIdx.x + blockIdx.x * blockDim.x</code> if we use 2D representation because that&rsquo;s the rules in CUDA. That&rsquo;s might be unintuitive at the beginning but you can think in this way: the thread should be continous along x then y, so in the matrix, the matrix is row-major, so the changing of x reflect the change of col. They are trying to improve cache locality.</p>
<h3 id="back-to-the-naive-kernel">Back to the naive kernel<a hidden class="anchor" aria-hidden="true" href="#back-to-the-naive-kernel">#</a></h3>
<p>After introduce the memory layout and rules it should be easy to write the kernel:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">kernel1</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="k">const</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">K</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span><span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">M</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">){</span> <span class="c1">// prevent over the boundary
</span></span></span><span class="line"><span class="cl">        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">            <span class="n">sum</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">C</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>After running the <code>make benchmark</code> we can check the result in the log, for the 8192 * 8192 matrix size, the computational timing and allocation timings are:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">Matrix size: 8192x8192 * 8192x8192
</span></span><span class="line"><span class="cl">------------------time used ----------------------
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Initialization time: 0.457656 seconds
</span></span><span class="line"><span class="cl">Computation time: 0.256279 seconds
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">--------------------------------------------------
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="can-we-make-it-better">Can we make it better?<a hidden class="anchor" aria-hidden="true" href="#can-we-make-it-better">#</a></h2>
<p>The answer is yes, recall how the array is stored in C++. It&rsquo;s row major, which means the memory address is continuous for each row.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">arr</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">}</span> <span class="p">,{</span> <span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">}</span> <span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="c1">// a[0][0] a[0][1] a[0][2] a[0][3] they are continuous.
</span></span></span><span class="line"><span class="cl"><span class="c1">// &amp;a[1][0] - &amp;a[0][0] = 4(stride)
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>So why don&rsquo;t we store B in col-major? It means we still take the logic <code>B[row][col]</code> but they are not align with their phsical location. Assuming the B is KxN matrix, then the <code>B[row][col] = B'[col * K + row]</code> in 1D.</p>
<p>The experienment can be done quickly in step-2, we change how we allocate memory to the matrix B(transpose it before do cuda memory copy):</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl">    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_B</span><span class="p">[</span><span class="n">j</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">j</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>remember we transpose B already so $B[j][i] = B^T[i][j] = B&rsquo;[i*K+j]$
$$ C_{ij} = \sum_k A_{ik} * B_{kj} = \sum_k A[row * K + i] * B[col * K + j] $$</p>
<p>The kernel is:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">kernel2</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="k">const</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">K</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span><span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">M</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">            <span class="n">sum</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">col</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">C</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>That&rsquo;s should work, let&rsquo;s see the result for the 8192 dimension compare to naive kernel:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">Matrix size: 8192x8192 * 8192x8192
</span></span><span class="line"><span class="cl">------------------time used ----------------------
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Initialization time: 0.554599 seconds
</span></span><span class="line"><span class="cl">Computation time: 1.65866 seconds
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">--------------------------------------------------
</span></span></code></pre></td></tr></table>
</div>
</div><p>Wait?!  Why it needs more time than naive kernel???</p>
<h3 id="analysis-of-memory-access-pattern">analysis of memory access pattern<a hidden class="anchor" aria-hidden="true" href="#analysis-of-memory-access-pattern">#</a></h3>
<p>We know the cache locality will define the calculation, why the pattern fail in this case?</p>
<p>The answer is we are using a lot of different threads to calculate it, each threads represents a different row, col. Inside a warp, all the threads ids are continuous, they are accessing the same row and different cols.</p>
<p>And inside a kernel, at the same time they are accessing the data: <code>sum += A[row * K + i] * B[col * K + i];</code>, different threads will use different col, and their stride is K which is 8192 and this cause all the threads could reuse data another threads access.</p>
<p>Warp has a feature called data coelesing, which means if threads inside a warp are accessing the continuous memory, they can combine the requests into a single one.</p>
<p>That explains why the performance dropped a lot, it improves the single threads efficiency but require too much bandwidth.</p>
<h3 id="try-1d-access-pattern">Try 1D access pattern<a hidden class="anchor" aria-hidden="true" href="#try-1d-access-pattern">#</a></h3>
<p>Open step-4 to do same experiment with step-1 but represent the matrix in 1D, you will find the performance improve a little bit, but not too much</p>
<h2 id="tiled-matrix-improve-the-data-transfer-efficiency-to-boost-computation">Tiled-matrix: Improve the data transfer efficiency to boost computation.<a hidden class="anchor" aria-hidden="true" href="#tiled-matrix-improve-the-data-transfer-efficiency-to-boost-computation">#</a></h2>
<p>After we know the properties of warp and block(all the threads in a block can use the same shared_memory), we can conside data reuse.</p>
<p>Recall what we have done in the naive kernel:
$$C[i][j] = \sum_k A[i][k] * B[k][j]$$</p>
<p>each thread use their own memory, which requires a row of A and a col of B, which is a huge waste. What can we do better? Tiled memory calculate, assuming we are taking a band of 16 x K of A and K x 16 of B to calculate the 16x16 of C_blocks matrix.</p>
<p>Of course we are using block matrix multiplication, we can&rsquo;t store the two bands of matrix together but when we iterate over K dimension, assuming the stride is 16, then we need a loop for K / 16 times, and each time we put the 16x16 A and 16x16 B tiles into shared memory to do mma add to C block(16x16).</p>
<p>How many times we can reuse it? 16 * 16 * 16 (calculation FLOPS) / 2 * 16 * 16 (the A tile and B tile) = 8 times, that&rsquo;s a huge boost, we can extend the number to 32, 64, etc, with larger tile. However, the tile can not expand infinitely because of the limitation of 100KB limitation(SRAM is very expensive!).</p>
<p>The best size in RTX 4090 is about 128x128 or 256 * 64, you can run and compare the step-6 and step-5 for comparison, of course in 5 the block size is 16 x 16 and it cause extra overhead to move the data into the shared memory and that offset the benefit of data reuse.</p>
<p>Here is the kernel for 32 x 32 C block size, you can try different block size and benchmark thems.</p>
<p>Notice the loading process is complete by all the threads in the blocks. You can use the size of A or B blocks / threads number to decide which thread load a certain or several elements of a matrix.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Kernel function to do naive matrix multiplication
</span></span></span><span class="line"><span class="cl"><span class="c1">// A[i][j] = i * N + j;
</span></span></span><span class="line"><span class="cl"><span class="c1">// B[i][j] = i * K + j;
</span></span></span><span class="line"><span class="cl"><span class="cp"># define TILE 32
</span></span></span><span class="line"><span class="cl"><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">kernel_tile</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="k">const</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">K</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// inside of a tile, local index
</span></span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">tx</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">ty</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// block index
</span></span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">bx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">by</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// global row and col index
</span></span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">by</span> <span class="o">*</span> <span class="n">TILE</span> <span class="o">+</span> <span class="n">ty</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">bx</span> <span class="o">*</span> <span class="n">TILE</span> <span class="o">+</span> <span class="n">tx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">As</span><span class="p">[</span><span class="n">TILE</span><span class="p">][</span><span class="n">TILE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">Bs</span><span class="p">[</span><span class="n">TILE</span><span class="p">][</span><span class="n">TILE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// loop over all tiles
</span></span></span><span class="line"><span class="cl">    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">K</span> <span class="o">+</span> <span class="n">TILE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">TILE</span><span class="p">;</span> <span class="o">++</span><span class="n">t</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// load A and B tiles into shared memory
</span></span></span><span class="line"><span class="cl">        <span class="c1">// each thread loads one element of each tile
</span></span></span><span class="line"><span class="cl">        <span class="k">if</span><span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">M</span> <span class="o">&amp;&amp;</span> <span class="n">t</span> <span class="o">*</span> <span class="n">TILE</span> <span class="o">+</span> <span class="n">tx</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">            <span class="n">As</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">t</span> <span class="o">*</span> <span class="n">TILE</span> <span class="o">+</span> <span class="n">tx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span><span class="k">else</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">As</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span><span class="p">(</span><span class="n">col</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">&amp;&amp;</span> <span class="n">t</span> <span class="o">*</span> <span class="n">TILE</span> <span class="o">+</span> <span class="n">ty</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">            <span class="n">Bs</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[(</span><span class="n">t</span> <span class="o">*</span> <span class="n">TILE</span> <span class="o">+</span> <span class="n">ty</span><span class="p">)</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span>  <span class="n">col</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span><span class="k">else</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">Bs</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// compute partial sum for this tile
</span></span></span><span class="line"><span class="cl">        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">TILE</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">            <span class="n">sum</span> <span class="o">+=</span> <span class="n">As</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">Bs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">tx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// write back the result
</span></span></span><span class="line"><span class="cl">    <span class="k">if</span><span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">M</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">        <span class="n">C</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="warp-level-matrix-multiplication---the-secrets-of-new-gpu-architecture">Warp level matrix multiplication - the secrets of new GPU architecture<a hidden class="anchor" aria-hidden="true" href="#warp-level-matrix-multiplication---the-secrets-of-new-gpu-architecture">#</a></h2>
<p>Currently the CUDA cores (which are the normal ALU elements) in the SM can do a lot of operations, but 90% of calculation in deep learning are matrix multiplication. Thus, a new warp-level of hardware is introduced, tensor core, we need use warp to schedule it.</p>
<p>Within a cycle, normal warp can do 64 FLOPS, but tensor core can do 512 FLOPS for a 16 x 16 matrix. (which is 8x boost.)</p>
<p>To use the kernel, we need to define the fragments(16x16) of A, B and C:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl">    <span class="c1">// define tensor core fragment
</span></span></span><span class="line"><span class="cl">    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_a</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">half</span><span class="p">,</span> <span class="n">wmma</span><span class="o">::</span><span class="n">row_major</span><span class="o">&gt;</span> <span class="n">a_frag</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_b</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">half</span><span class="p">,</span> <span class="n">wmma</span><span class="o">::</span><span class="n">row_major</span><span class="o">&gt;</span> <span class="n">b_frag</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">accumulator</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="kt">float</span><span class="o">&gt;</span> <span class="n">c_frag</span><span class="p">;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>F
At this stage, we should divide a sub sub matrix in C for the warp to control, for simplicity, we assume the block size is 16 x 16 and the warp control all the block, then we can have the kernel to boost the 16 x 16 tiled matrix multiplication by:</p>
<p>$$C_sub = A[16][:] @ B[:][16] = A[16][0..15] @ [0..15][16] +  A[16][16..31] @ [16..31][16] + &hellip;$$</p>
<p>Here is the kernel:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;mma.h&gt;</span><span class="cp"> </span><span class="c1">// for wmma
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;cuda_fp16.h&gt;</span><span class="cp"> </span><span class="c1">// for half
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="k">namespace</span> <span class="n">nvcuda</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Kernel function to do naive matrix multiplication
</span></span></span><span class="line"><span class="cl"><span class="c1">// A[i][j] = i * N + j;
</span></span></span><span class="line"><span class="cl"><span class="c1">// B[i][j] = i * K + j;
</span></span></span><span class="line"><span class="cl"><span class="cp"># define TILE 16
</span></span></span><span class="line"><span class="cl"><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">kernel_tile</span><span class="p">(</span><span class="k">const</span> <span class="n">half</span> <span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="k">const</span> <span class="n">half</span> <span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="k">const</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">K</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1">// define tensor core fragment
</span></span></span><span class="line"><span class="cl">    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_a</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">half</span><span class="p">,</span> <span class="n">wmma</span><span class="o">::</span><span class="n">row_major</span><span class="o">&gt;</span> <span class="n">a_frag</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_b</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">half</span><span class="p">,</span> <span class="n">wmma</span><span class="o">::</span><span class="n">row_major</span><span class="o">&gt;</span> <span class="n">b_frag</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">accumulator</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="n">TILE</span><span class="p">,</span> <span class="kt">float</span><span class="o">&gt;</span> <span class="n">c_frag</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// initialization of C fragment
</span></span></span><span class="line"><span class="cl">    <span class="n">wmma</span><span class="o">::</span><span class="n">fill_fragment</span><span class="p">(</span><span class="n">c_frag</span><span class="p">,</span> <span class="mf">0.0f</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">block_row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">block_col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="n">half</span> <span class="o">*</span><span class="n">A_ptr</span> <span class="o">=</span> <span class="n">A</span> <span class="o">+</span> <span class="n">block_row</span> <span class="o">*</span> <span class="n">TILE</span> <span class="o">*</span> <span class="n">K</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="n">half</span> <span class="o">*</span><span class="n">B_ptr</span> <span class="o">=</span> <span class="n">B</span> <span class="o">+</span> <span class="n">block_col</span> <span class="o">*</span> <span class="n">TILE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">TILE</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// load the intput matrices A and B to fragments
</span></span></span><span class="line"><span class="cl">        <span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">a_frag</span><span class="p">,</span> <span class="n">A_ptr</span><span class="p">,</span> <span class="n">K</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">b_frag</span><span class="p">,</span> <span class="n">B_ptr</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// perform the matrix multiplication
</span></span></span><span class="line"><span class="cl">        <span class="n">wmma</span><span class="o">::</span><span class="n">mma_sync</span><span class="p">(</span><span class="n">c_frag</span><span class="p">,</span> <span class="n">a_frag</span><span class="p">,</span> <span class="n">b_frag</span><span class="p">,</span> <span class="n">c_frag</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// calculate the starting row and column index of C matrix
</span></span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="o">*</span> <span class="n">c_ptr</span> <span class="o">=</span> <span class="n">C</span> <span class="o">+</span> <span class="n">block_row</span> <span class="o">*</span> <span class="n">TILE</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">block_col</span> <span class="o">*</span> <span class="n">TILE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// write memory back to C
</span></span></span><span class="line"><span class="cl">    <span class="c1">// fix the error: no matching function for call to &#39;store_matrix_sync&#39;
</span></span></span><span class="line"><span class="cl">    <span class="n">wmma</span><span class="o">::</span><span class="n">store_matrix_sync</span><span class="p">(</span><span class="n">c_ptr</span><span class="p">,</span> <span class="n">c_frag</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">wmma</span><span class="o">::</span><span class="n">mem_row_major</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="final-steps---combine-warp-level-matrix-multiplication-and-shared-memory">Final steps - combine warp level matrix multiplication and shared memory<a hidden class="anchor" aria-hidden="true" href="#final-steps---combine-warp-level-matrix-multiplication-and-shared-memory">#</a></h2>
<p>In this way, we know how to use data better to boost the bandwidth. Then the wmma can be used with shared memory, in this case we use 128 x 128 x 32 kernel (A tile is 128 x 32, B tile is 32 x 128 and C tile is 128 x 128 to save shared memory). The shared memory usage is 128 * 32 * 2 = 8192 elements and 16KB usage(for half data type is 2 Bytes) which is pretty good, it will not pressure L1 cache a lot and can hold 2-4 blocks in a SM.</p>
<p>Here is the kernel, notice each block control 128 x 128 matrix, and 16 warps will consumes this. Each warp get 32 x 32 warp level tile matrix, it should be divided into 4 fragments matrix and do the calculation. Here is the kernel, you can also try it in step-12.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;mma.h&gt;</span><span class="cp"> </span><span class="c1">// for wmma
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;cuda_fp16.h&gt;</span><span class="cp"> </span><span class="c1">// for half
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="k">namespace</span> <span class="n">nvcuda</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// warp size wichi is constant 16 x 16 x 16
</span></span></span><span class="line"><span class="cl"><span class="cp">#define WARP_M 16
</span></span></span><span class="line"><span class="cl"><span class="cp">#define WARP_N 16
</span></span></span><span class="line"><span class="cl"><span class="cp">#define WARP_K 16
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// block tile size
</span></span></span><span class="line"><span class="cl"><span class="cp">#define BLOCK_M 128 </span><span class="c1">// 8 * WARP_M
</span></span></span><span class="line"><span class="cl"><span class="cp">#define BLOCK_N 128 </span><span class="c1">// 8 * WARP_N
</span></span></span><span class="line"><span class="cl"><span class="cp">#define BLOCK_K 32  </span><span class="c1">// 8 * WARP_K 
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// 
</span></span></span><span class="line"><span class="cl"><span class="cp">#define WARPS_PER_BLOCK_M 4 </span><span class="c1">// block number of warps in M dimension
</span></span></span><span class="line"><span class="cl"><span class="cp">#define WARPS_PER_BLOCK_N 4 </span><span class="c1">// block number of warps in N dimension
</span></span></span><span class="line"><span class="cl"><span class="cp">#define BLOCK_WARPS (WARPS_PER_BLOCK_M * WARPS_PER_BLOCK_N) </span><span class="c1">// 4 * 4 = 16 warps per block
</span></span></span><span class="line"><span class="cl"><span class="cp">#define BLOCK_THREADS (32 * BLOCK_WARPS) </span><span class="c1">// 32 * 16 = 512 threads per block
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// the govern region of C_blocks for each warp
</span></span></span><span class="line"><span class="cl"><span class="cp">#define WARP_C_TILE_M (BLOCK_M / WARPS_PER_BLOCK_M) </span><span class="c1">// 128 / 4 = 32
</span></span></span><span class="line"><span class="cl"><span class="cp">#define WARP_C_TILE_N (BLOCK_N / WARPS_PER_BLOCK_N) </span><span class="c1">// 128 / 4 = 32
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// number of fragments of C for each warp
</span></span></span><span class="line"><span class="cl"><span class="cp">#define WARP_C_FRAGS_M (WARP_C_TILE_M / WARP_M) </span><span class="c1">// 32 / 16 = 2
</span></span></span><span class="line"><span class="cl"><span class="cp">#define WARP_C_FRAGS_N (WARP_C_TILE_N / WARP_N) </span><span class="c1">// 32 / 16 = 2
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">kernel_smem_tile_128x128</span><span class="p">(</span><span class="k">const</span> <span class="n">half</span> <span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="k">const</span> <span class="n">half</span> <span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                         <span class="k">const</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">K</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// A: (M, K) row-major
</span></span></span><span class="line"><span class="cl">    <span class="c1">// B: (K, N) col-major
</span></span></span><span class="line"><span class="cl">    <span class="c1">// C: (M, N) row-major
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// shared memory size for A and B tiles 4096 + 4,096 = 8,192 elements
</span></span></span><span class="line"><span class="cl">    <span class="c1">// 16KB in total.
</span></span></span><span class="line"><span class="cl">    <span class="c1">// A tile: (BLOCK_M, BLOCK_K) -&gt; (128, 32) = 4096
</span></span></span><span class="line"><span class="cl">    <span class="n">__shared__</span> <span class="n">half</span> <span class="n">smem_a</span><span class="p">[</span><span class="n">BLOCK_M</span> <span class="o">*</span> <span class="n">BLOCK_K</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// B tile: (BLOCK_K, BLOCK_N) -&gt; (32, 128) = 4096
</span></span></span><span class="line"><span class="cl">    <span class="n">__shared__</span> <span class="n">half</span> <span class="n">smem_b</span><span class="p">[</span><span class="n">BLOCK_K</span> <span class="o">*</span> <span class="n">BLOCK_N</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1">// fragments of A and B
</span></span></span><span class="line"><span class="cl">    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_a</span><span class="p">,</span> <span class="n">WARP_M</span><span class="p">,</span> <span class="n">WARP_N</span><span class="p">,</span> <span class="n">WARP_K</span><span class="p">,</span> <span class="n">half</span><span class="p">,</span> <span class="n">wmma</span><span class="o">::</span><span class="n">row_major</span><span class="o">&gt;</span> <span class="n">a_frag</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_b</span><span class="p">,</span> <span class="n">WARP_M</span><span class="p">,</span> <span class="n">WARP_N</span><span class="p">,</span> <span class="n">WARP_K</span><span class="p">,</span> <span class="n">half</span><span class="p">,</span> <span class="n">wmma</span><span class="o">::</span><span class="n">col_major</span><span class="o">&gt;</span> <span class="n">b_frag</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// fragmesnts of C (in total 8 fragments per warp)
</span></span></span><span class="line"><span class="cl">    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">accumulator</span><span class="p">,</span> <span class="n">WARP_M</span><span class="p">,</span> <span class="n">WARP_N</span><span class="p">,</span> <span class="n">WARP_K</span><span class="p">,</span> <span class="kt">float</span><span class="o">&gt;</span> <span class="n">c_frag</span><span class="p">[</span><span class="n">WARP_C_FRAGS_M</span><span class="p">][</span><span class="n">WARP_C_FRAGS_N</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// initialization of C fragments in each warp
</span></span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">m</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">m</span> <span class="o">&lt;</span> <span class="n">WARP_C_FRAGS_M</span><span class="p">;</span> <span class="o">++</span><span class="n">m</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="n">WARP_C_FRAGS_N</span><span class="p">;</span> <span class="o">++</span><span class="n">n</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">wmma</span><span class="o">::</span><span class="n">fill_fragment</span><span class="p">(</span><span class="n">c_frag</span><span class="p">[</span><span class="n">m</span><span class="p">][</span><span class="n">n</span><span class="p">],</span> <span class="mf">0.0f</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 4 x 4 warps per block
</span></span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">warpId</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span> <span class="c1">// 0..15
</span></span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">laneId</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="c1">// 0..31
</span></span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">block_row_base</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">BLOCK_M</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">block_col_base</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">BLOCK_N</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1">// warpId 0, 32, 64 
</span></span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">warp_row_offset</span> <span class="o">=</span> <span class="p">(</span><span class="n">warpId</span> <span class="o">/</span> <span class="n">WARPS_PER_BLOCK_N</span><span class="p">)</span> <span class="o">*</span> <span class="n">WARP_C_TILE_M</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">    <span class="c1">// 0, 32, 64 
</span></span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">warp_col_offset</span> <span class="o">=</span> <span class="p">(</span><span class="n">warpId</span> <span class="o">%</span> <span class="n">WARPS_PER_BLOCK_N</span><span class="p">)</span> <span class="o">*</span> <span class="n">WARP_C_TILE_N</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// thread ID within the block for loading shared memory
</span></span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="c1">// 0..511
</span></span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k_tile_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k_tile_idx</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">k_tile_idx</span> <span class="o">+=</span> <span class="n">BLOCK_K</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">const</span> <span class="n">half</span> <span class="o">*</span><span class="n">A_gmem_ptr</span> <span class="o">=</span> <span class="n">A</span> <span class="o">+</span> <span class="n">block_row_base</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">k_tile_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">const</span> <span class="n">half</span> <span class="o">*</span><span class="n">B_gmem_ptr</span> <span class="o">=</span> <span class="n">B</span> <span class="o">+</span> <span class="n">block_col_base</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">k_tile_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// A (row-major) load
</span></span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">BLOCK_M</span> <span class="o">*</span> <span class="n">BLOCK_K</span> <span class="o">/</span> <span class="n">BLOCK_THREADS</span><span class="p">);</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// 4096 / 512 = 8
</span></span></span><span class="line"><span class="cl">            <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">tid</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">BLOCK_THREADS</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">/</span> <span class="n">BLOCK_K</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">%</span> <span class="n">BLOCK_K</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">smem_a</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">A_gmem_ptr</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// B (col-major) load
</span></span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">BLOCK_K</span> <span class="o">*</span> <span class="n">BLOCK_N</span> <span class="o">/</span> <span class="n">BLOCK_THREADS</span><span class="p">);</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// 4096 / 512 = 8
</span></span></span><span class="line"><span class="cl">            <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">tid</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">BLOCK_THREADS</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">/</span> <span class="n">BLOCK_N</span><span class="p">;</span> <span class="c1">// (k-dim)
</span></span></span><span class="line"><span class="cl">            <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">%</span> <span class="n">BLOCK_N</span><span class="p">;</span> <span class="c1">// (n-dim)
</span></span></span><span class="line"><span class="cl">            <span class="n">smem_b</span><span class="p">[</span><span class="n">col</span> <span class="o">*</span> <span class="n">BLOCK_K</span> <span class="o">+</span> <span class="n">row</span><span class="p">]</span> <span class="o">=</span> <span class="n">B_gmem_ptr</span><span class="p">[</span><span class="n">col</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">row</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1">// K-loop (inner): BLOCK_K (32) / WARP_K (16) = 2 
</span></span></span><span class="line"><span class="cl">        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">inner_k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">inner_k</span> <span class="o">&lt;</span> <span class="n">BLOCK_K</span><span class="p">;</span> <span class="n">inner_k</span> <span class="o">+=</span> <span class="n">WARP_K</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1">// calculate all fragments of A(16 x 32) and B(32 x 16) for this warp to compute a C fragment(16x16)
</span></span></span><span class="line"><span class="cl">            <span class="c1">// so there are total 2 times of wmma iuoperations per 
</span></span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">m</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">m</span> <span class="o">&lt;</span> <span class="n">WARP_C_FRAGS_M</span><span class="p">;</span> <span class="o">++</span><span class="n">m</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// 0..
</span></span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="n">WARP_C_FRAGS_N</span><span class="p">;</span> <span class="o">++</span><span class="n">n</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// 0..1
</span></span></span><span class="line"><span class="cl">                    
</span></span><span class="line"><span class="cl">                    <span class="c1">// A tile for this warp starts at smem_a[warp_row_offset * BLOCK_K]
</span></span></span><span class="line"><span class="cl">                    <span class="c1">// M-dim offset: m * WARP_M
</span></span></span><span class="line"><span class="cl">                    <span class="k">const</span> <span class="n">half</span> <span class="o">*</span> <span class="n">A_smem_ptr</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">smem_a</span><span class="p">[</span> <span class="p">(</span><span class="n">warp_row_offset</span> <span class="o">+</span> <span class="n">m</span> <span class="o">*</span> <span class="n">WARP_M</span><span class="p">)</span> <span class="o">*</span> <span class="n">BLOCK_K</span> <span class="o">+</span> <span class="n">inner_k</span> <span class="p">];</span>
</span></span><span class="line"><span class="cl">                    
</span></span><span class="line"><span class="cl">                    <span class="c1">// B tile for this warp starts at smem_b[warp_col_offset * BLOCK_K]
</span></span></span><span class="line"><span class="cl">                    <span class="c1">// N-dim offset: n * WARP_N
</span></span></span><span class="line"><span class="cl">                    <span class="k">const</span> <span class="n">half</span> <span class="o">*</span> <span class="n">B_smem_ptr</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">smem_b</span><span class="p">[</span> <span class="p">(</span><span class="n">warp_col_offset</span> <span class="o">+</span> <span class="n">n</span> <span class="o">*</span> <span class="n">WARP_N</span><span class="p">)</span> <span class="o">*</span> <span class="n">BLOCK_K</span> <span class="o">+</span> <span class="n">inner_k</span> <span class="p">];</span>
</span></span><span class="line"><span class="cl">                    
</span></span><span class="line"><span class="cl">                    <span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">a_frag</span><span class="p">,</span> <span class="n">A_smem_ptr</span><span class="p">,</span> <span class="n">BLOCK_K</span><span class="p">);</span> <span class="c1">// LDM=BLOCK_K (32)
</span></span></span><span class="line"><span class="cl">                    <span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">b_frag</span><span class="p">,</span> <span class="n">B_smem_ptr</span><span class="p">,</span> <span class="n">BLOCK_K</span><span class="p">);</span> <span class="c1">// LDM=BLOCK_K (32)
</span></span></span><span class="line"><span class="cl">                    
</span></span><span class="line"><span class="cl">                    <span class="c1">// accumulate
</span></span></span><span class="line"><span class="cl">                    <span class="n">wmma</span><span class="o">::</span><span class="n">mma_sync</span><span class="p">(</span><span class="n">c_frag</span><span class="p">[</span><span class="n">m</span><span class="p">][</span><span class="n">n</span><span class="p">],</span> <span class="n">a_frag</span><span class="p">,</span> <span class="n">b_frag</span><span class="p">,</span> <span class="n">c_frag</span><span class="p">[</span><span class="n">m</span><span class="p">][</span><span class="n">n</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">                <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">__syncthreads</span><span class="p">();</span> <span class="c1">// synchronize before loading the next tile
</span></span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// write back C fragments to global memory
</span></span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="o">*</span> <span class="n">c_ptr_base</span> <span class="o">=</span> <span class="n">C</span> <span class="o">+</span> <span class="p">(</span><span class="n">block_row_base</span> <span class="o">+</span> <span class="n">warp_row_offset</span><span class="p">)</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">block_col_base</span> <span class="o">+</span> <span class="n">warp_col_offset</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1">// write back all C fragments of this warp
</span></span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">m</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">m</span> <span class="o">&lt;</span> <span class="n">WARP_C_FRAGS_M</span><span class="p">;</span> <span class="o">++</span><span class="n">m</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="n">WARP_C_FRAGS_N</span><span class="p">;</span> <span class="o">++</span><span class="n">n</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="kt">float</span><span class="o">*</span> <span class="n">c_ptr</span> <span class="o">=</span> <span class="n">c_ptr_base</span> <span class="o">+</span> <span class="p">(</span><span class="n">m</span> <span class="o">*</span> <span class="n">WARP_M</span><span class="p">)</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">WARP_N</span><span class="p">);</span> <span class="c1">// LDM = N
</span></span></span><span class="line"><span class="cl">            <span class="n">wmma</span><span class="o">::</span><span class="n">store_matrix_sync</span><span class="p">(</span><span class="n">c_ptr</span><span class="p">,</span> <span class="n">c_frag</span><span class="p">[</span><span class="n">m</span><span class="p">][</span><span class="n">n</span><span class="p">],</span> <span class="n">N</span><span class="p">,</span> <span class="n">wmma</span><span class="o">::</span><span class="n">mem_row_major</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Very interesting facts: 64 x 64 x 64 is the same as 128 * 32 * 128, but we can get a larger C blocks in the second strategy, which is 128 x 128 and this give us more benefits, we prefer to use panel size of tiled matrix to maxmiaze the compututation under a certain amount of bandwidth.</p>
<h2 id="what-could-be-done-next">What could be done next?<a hidden class="anchor" aria-hidden="true" href="#what-could-be-done-next">#</a></h2>
<p>As for now, you should have a good understanding of the CUDA and GPU meomry hierachy, but we could still optimize it with async data fetching and double buffering(latency hiding). I will cover it in the later articles.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="next" href="http://localhost:1313/posts/2025-09-19-add-array-support-for-dsl-based-on-minimal-cpu/">
    <span class="title">Next »</span>
    <br>
    <span>Add array support for DSL on Minimal CPU</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="http://localhost:1313/">Xi&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
